{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "sfk8Zrul_E8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Citation to the original paper:\n",
        "\n",
        "1.   Al-Rabeah, M.H., Lakizadeh, A. Prediction of drug-drug interaction events using graph neural networks based feature extraction. Sci Rep 12, 15590 (2022). https://doi.org/10.1038/s41598-022-19999-4\n",
        "\n",
        "Link to the original paper’s repo: https://github.com/Mohammad-Hussain95/GNN_DDI\n",
        "\n",
        "\n",
        "Link to Video: https://mediaspace.illinois.edu/media/t/1_gg81lr8i\n",
        "\n",
        "Link to public repo: https://github.com/genggeng88/dl4h_team61\n"
      ],
      "metadata": {
        "id": "KgbcP3kJP2vq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "* Background of the problem\n",
        "  * Type of problem:\n",
        "    * Nowadays, many difficult diseases are treated using drug mixes (Polypharmacy), which is a good approach that utilizes the synergistic effects of drug interactions. However, unplanned DDIs could risk a patient's life because they may cause side effects or perhaps dangerous toxicity. The detection of DDIs becomes much more necessary, but diagnosing DDI on a large number of drug pairs, both in vitro and in vivo, is costly and time-consuming.\n",
        "  * Importance/meaning of solving the problem:\n",
        "    * Detecting possible DDIs decreases the incidence of unexpected drug interactions and reduces drug production costs. It also can optimize the drug creation process.\n",
        "  * Difficulty of the problem:\n",
        "    *  DDI network can provide vital information about drugs interactions. Furthermore, using an attributed heterogeneous DDIs network that presents the drug's interaction types along with the drug features can better demonstrate the intrinsic characteristics of a drug. However, it is challenging to integrate various features effectively because the drug features might be correlated and contain redundant information.\n",
        "  * State of the art methods and effectiveness:\n",
        "    * There are four popular approaches in the DDI prediction field: Similarity-based methods, Matrix Factorization-based methods, network analysis-based methods and Deep Learning-based methods. Most of current methods are developed to predict whether drugs interact or not, but not to predict the DDI events. Even though few researches created strong efforts in event prediction but there is a space for advancement.\n",
        "*  Paper explanation\n",
        "  * What did the paper propose:\n",
        "    * The paper proposed a method for predicting DDI and their type (event) based on attributed heterogeneous graph embedding and a deep learning approach.\n",
        "  * What is the innovations of the method:\n",
        "    * This paper first built a heterogeneous drug network by integrating the drug properties in each type of DDI. It then made predictions on what kind of interaction is between drugs.\n",
        "  * How well the proposed method work:\n",
        "    * Models like DDIMDL, CNN-DDI, DANN-DDI, MDNN, RF, KNN, LR were used for performance comparison with GNN-DDI base on the metrics like ACC, AUPR, AUC, F1 Score, Precision, and Recall. GNN-DDI showed the best performance among different models.\n",
        "  * What is the contribution to the reasearch regime:\n",
        "    * This paper proposed a new approach in generating heterogeneous drug network. It also helped make predictions on not only drug interaction, but also drug interaction events which can be beneficial to patients and drug production.\n"
      ],
      "metadata": {
        "id": "MQ0sNuMePBXx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scope of Reproducibility:\n",
        "\n",
        "1. Impact of Embedding Dimension Size:\n",
        "    - Hypothesis: The size of the embedding dimension affects the model's performance, with a dimension size of 32 leading to the best accuracy. Dimension sizes of 16 and 64 yield slightly inferior performance.\n",
        "    - Method: Train the model using different embedding dimension sizes (16, 32, 64) and compare their performance metrics.\n",
        "\n",
        "3. Impact of Different Drug Feature Matrices:\n",
        "    - Hypothesis: Using different drug feature matrices (similarity matrices) displays varying performance on the proposed model. The combined feature matrix shows the best performance.\n",
        "    - Method: Test the model's performance using different drug feature matrices and assess the effectiveness of each matrix in predicting DDIs.\n",
        "4. Comparison with Existing Models:\n",
        "    - Hypothesis: The proposed method outperforms existing models such as DNN, RF, KNN, and LR in predicting DDIs.\n",
        "    - Method: Compare the performance of the proposed method with existing models (DNN, RF, KNN, LR) using common performance metrics and assess its superiority."
      ],
      "metadata": {
        "id": "uygL9tTPSVHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methodology\n",
        "\n",
        "Environment:\n",
        "*   Python 3.6.12\n",
        "*   Tensorflow 2.1\n",
        "*   keras 2.3\n",
        "*   numpy 1.19\n",
        "*   pandas 1.1.3\n",
        "*   sqlite3 3.8.6\n",
        "*   tqdm 4.63\n",
        "*   matplotlib 3.3\n",
        "\n"
      ],
      "metadata": {
        "id": "xWAHJ_1CdtaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing packages\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "import sqlite3\n",
        "from google.colab import drive\n",
        "sys.path.append(os.path.abspath('/content/drive/MyDrive'))"
      ],
      "metadata": {
        "id": "yu61Jp1xrnKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##   Process Overview\n",
        "\n",
        "The proposed model contains two stages:\n",
        "\n",
        "1. Collect drug information from different sources and integrate them through the formation of attributed heterogeneous networks. Then use a Graph Neural Network to generate embedding vectors based on different drug interaction types and drug attributes.\n",
        "\n",
        "The following figure provides an overview on the first stage:<br>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1uda8ODPHVBPSWiFK62fiQc1Qij2XsN-M\" width=\"450\" height=\"300\">\n",
        "\n",
        "2. Aggregate the representation vectors and use a fully connected neural network to predict the DDIs.\n",
        "\n",
        "The following figure provides an overview on the second stage:<br>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1BbsygQJHpvRCFetVN67ebZixQTLMJBRd\" width=\"450\" height=\"300\">\n"
      ],
      "metadata": {
        "id": "3muyDPFPbozY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Data\n",
        "In this project, we obtained the data from a [sqlite3 database](https://github.com/YifanDengWHU/DDIMDL) which was compiled from [DrugBank](https://go.drugbank.com/) 5.1.3 verision. It has 4 tables:\n",
        "1. <b>drug</b> contains 572 kinds of drugs and their target, enzyme, pathway, and SMILES features.\n",
        "2. <b>event</b> contains the 37264 DDIs between the 572 kinds of drugs.\n",
        "3. <b>extraction</b> is the process result of NLPProcess. Each interaction is transformed to a tuple: {mechanism, action, drugA, drugB}\n",
        "4. <b>event_numer</b> lists the kinds of DDI events and their occurence frequency.\n",
        "\n"
      ],
      "metadata": {
        "id": "2NbPHUTMbkD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We first import the data from the paper's provided database."
      ],
      "metadata": {
        "id": "x0NS5sjopgDV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the tables from sqlite3 database called \"Event.db\"\n",
        "\n",
        "#Connection to the DB\n",
        "conn = sqlite3.connect('/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/event.db')\n",
        "\n",
        "cursor = conn. cursor()\n",
        "cursor. execute(\"SELECT name FROM sqlite_master WHERE type='table';\") # [('event_number',), ('event',), ('drug',), ('extraction',)]\n",
        "print(\"Tables : \",cursor. fetchall())\n",
        "cursor. execute(\"SELECT name FROM PRAGMA_TABLE_INFO('drug');\") # [('index',), ('id',), ('target',), ('enzyme',), ('pathway',), ('smile',), ('name',)]\n",
        "print(\"drug : \",cursor. fetchall())\n",
        "cursor. execute(\"SELECT name FROM PRAGMA_TABLE_INFO('event');\") # [('index',), ('id',), ('target',), ('enzyme',), ('pathway',), ('smile',), ('name',)]\n",
        "print(\"event : \",cursor. fetchall())\n",
        "cursor. execute(\"SELECT COUNT(*) FROM event\") # [('index',), ('id',), ('target',), ('enzyme',), ('pathway',), ('smile',), ('name',)]\n",
        "print(\"event row COUNT: \",cursor. fetchall())\n",
        "\n",
        "# close the DB connection\n",
        "conn.close()"
      ],
      "metadata": {
        "id": "mXWdIh5n_7Oo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For each drug feature in the data, we generate a similarity matrix to be used as an attribute in our Drug to Drug interaction networks."
      ],
      "metadata": {
        "id": "CcglzXBiK_Ib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate similarity matrices\n",
        "import csv\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "def save_f(name,mat):\n",
        "  print(name)\n",
        "  print(mat.shape)\n",
        "  df = pd.DataFrame(mat)\n",
        "  df.to_csv('/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/'+str(name)+'.csv', header=None, index=False)\n",
        "\n",
        "# construct similarity matrices from adjacency matrices of the properties useing the Jacquard similarity function\n",
        "def Jaccard(matrix):\n",
        "  matrix = np.mat(matrix)\n",
        "  numerator = matrix * matrix.T\n",
        "  denominator = np.ones(np.shape(matrix)) * matrix.T + matrix * np.ones(np.shape(matrix.T)) - matrix * matrix.T\n",
        "  return numerator / denominator\n",
        "\n",
        "\n",
        "# generate the feature vector\n",
        "def feature_vector(df, feature_name):\n",
        "  all_feature = []\n",
        "  drug_list = np.array(df[feature_name]).tolist()\n",
        "  # Features for each drug, for example, when feature_name is target, drug_list=[\"P30556|P05412\",\"P28223|P46098|……\"]\n",
        "  for i in drug_list:\n",
        "    for each_feature in i.split('|'):\n",
        "      if each_feature not in all_feature:\n",
        "        all_feature.append(each_feature)  # obtain all the features\n",
        "  feature_matrix = np.zeros((len(drug_list), len(all_feature)), dtype=float)\n",
        "  df_feature = DataFrame(feature_matrix, columns=all_feature)  # Consrtuct feature matrices with key of dataframe\n",
        "  for i in range(len(drug_list)):\n",
        "    for each_feature in df[feature_name].iloc[i].split('|'):\n",
        "      df_feature[each_feature].iloc[i] = 1\n",
        "\n",
        "  sim_matrix = np.asarray(Jaccard(np.array(df_feature)))\n",
        "  sim_matrix1 = np.array(sim_matrix)\n",
        "  pca = PCA(n_components=len(sim_matrix1))  # PCA dimension\n",
        "  pca.fit(sim_matrix)\n",
        "  sim_matrix = pca.transform(sim_matrix)\n",
        "\n",
        "  return sim_matrix\n",
        "\n",
        "conn = sqlite3.connect('/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/event.db')\n",
        "df_drug = pd.read_sql('select * from drug;', conn)\n",
        "\n",
        "feature_list = ['target', 'enzyme', 'pathway', 'smile']\n",
        "\n",
        "print(df_drug[feature_list[0]][:2])\n",
        "print(df_drug[:][:2])\n",
        "\n",
        "drugs = df_drug[:]\n",
        "drugs = np.array(np.vstack((df_drug['index'],df_drug['name'])))\n",
        "drugs = drugs.T\n",
        "\n",
        "# save the four individual feature matrix\n",
        "for feature in feature_list:\n",
        "  mat = feature_vector(df_drug, feature)\n",
        "  save_f(feature+\"_PCA\", mat)\n",
        "\n",
        "conn.close()"
      ],
      "metadata": {
        "id": "zk7iyktLh32z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using our event and extraction data, we create the Drug-Drug Interaction network that will be used to create Drug-to-Event-type embedding vectors for each feature."
      ],
      "metadata": {
        "id": "lZgghGwoKk-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# retrieve records from the extraction table in the database\n",
        "# and construct the DDI Matrix\n",
        "\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import itertools\n",
        "\n",
        "conn = sqlite3.connect('/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/event.db')\n",
        "extraction = pd.read_sql('select * from extraction;', conn)\n",
        "mechanism = extraction['mechanism']\n",
        "action = extraction['action']\n",
        "drugA = extraction['drugA']\n",
        "drugB = extraction['drugB']\n",
        "d_label = {}\n",
        "d_event=[]\n",
        "for i in range(len(mechanism)):\n",
        "  d_event.append(mechanism[i]+\" \"+action[i])\n",
        "count={}\n",
        "for i in d_event:\n",
        "  if i in count:\n",
        "    count[i]+=1\n",
        "  else:\n",
        "    count[i]=1\n",
        "list1 = sorted(count.items(), key=lambda x: x[1],reverse=True)\n",
        "for i in range(len(list1)):\n",
        "  d_label[list1[i][0]]=i\n",
        "\n",
        "DDI=[]\n",
        "for i in range(len(d_event)):\n",
        "  DDI.append(np.hstack((d_label[d_event[i]],drugA[i], drugB[i])))\n",
        "\n",
        "mat_DDI = np.array(DDI)\n",
        "key = drugs[:,1]\n",
        "val = drugs[:,0]\n",
        "dic = dict(zip(key,val))\n",
        "postive1 = [dic[item] for item in mat_DDI[:,1]]\n",
        "postive2 = [dic[item] for item in mat_DDI[:,2]]\n",
        "full_pos = np.array(np.vstack((mat_DDI[:,0],postive1,postive2))).astype('int32')\n",
        "full_pos = full_pos.T\n",
        "\n",
        "df = pd.DataFrame(np.array(full_pos).tolist())\n",
        "df.to_csv('/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/full_pos2.txt', header=None, index=None, sep=' ')\n",
        "\n",
        "conn.close()"
      ],
      "metadata": {
        "id": "sUx4yBW6i8et"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we generate negative drug pairings for use in training the Graph Neural Network."
      ],
      "metadata": {
        "id": "BOBX8j6kKc4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate negative pairs (non-interacting drug pairs) from a given matrix of positive pairs (interacting drug pairs).\n",
        "\n",
        "def make_neg_pairs(matrix):\n",
        "  all_pos = np.array(matrix)\n",
        "  s1=np.unique(all_pos[:,0])\n",
        "  s2=np.unique(all_pos[:,1])\n",
        "  s3=set(s1).union(s2)\n",
        "  conncted_drug = sorted(s3)\n",
        "  print(\"there are \", len(s3), \" drugs have connection out of 572\")\n",
        "  ss = [ii for ii in range(572) if not ii in s3]\n",
        "  print(\"there are \", len(ss), \" drugs without connection out of 572\")\n",
        "  pairs_false = list()\n",
        "  pairs = list()\n",
        "  comparing = all_pos\n",
        "  print(\"start callcolate combinations ... \")\n",
        "  for dr1,dr2 in itertools.combinations(conncted_drug,2):\n",
        "    d1=np.array([dr1,dr2])\n",
        "    d2=np.array([dr1,dr2])\n",
        "    if dr1 == dr2: continue\n",
        "    else: pairs.append((dr1,dr2))\n",
        "  print(\"all pairs : \",len(pairs))\n",
        "  for dr in tqdm(pairs, desc=\"pairs_false generating : \"):\n",
        "    d1=np.array([dr[0],dr[1]])\n",
        "    d2=np.array([dr[1],dr[0]])\n",
        "    if not (dr[0]==dr[1]):\n",
        "      if not ((d2 == comparing).all(axis=1).any() or (d1 == comparing).all(axis=1).any()):\n",
        "        pairs_false.append([dr[0],dr[1]])\n",
        "  base=[]\n",
        "  base2=[]\n",
        "  for o in tqdm(pairs_false, \"all_neg generating : \"):\n",
        "    if (not any(o[0] in h for h in base)) or (not any(o[1] in h for h in base)):\n",
        "      base.append(o)\n",
        "    else:\n",
        "      base2.append(o)\n",
        "  if len(base) > len(conncted_drug) :\n",
        "    print(\"less base .... !\")\n",
        "  pairs_f1 = np.array(base2)\n",
        "  np.random.shuffle(pairs_f1)\n",
        "  all_neg = np.concatenate((base,pairs_f1[:len(all_pos)-len(base)]),axis=0)\n",
        "  np.random.shuffle(all_neg)\n",
        "  print(\"all_neg.shape : \", all_neg.shape, \"all_pos.shape : \", all_pos.shape)\n",
        "  df = pd.DataFrame(np.array(all_neg).tolist())\n",
        "  df.to_csv('/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/all_neg2.txt', header=None, index=None, sep=' ')\n",
        "  return all_neg\n",
        "\n",
        "all_neg = make_neg_pairs(full_pos[:,1:])"
      ],
      "metadata": {
        "id": "1_zVgYcajOoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we prepare train, test, and validation subsets for our GNN model Training."
      ],
      "metadata": {
        "id": "AgO0tttMKQ7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare the train and test subsets\n",
        "\n",
        "for itm in tqdm(range(9)):\n",
        "  if itm == 0:\n",
        "    full_pos = np.array(np.array(pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/full_pos2.txt\", header=None , sep=' ')).tolist())\n",
        "  elif itm == 1:\n",
        "    all_neg = np.array(np.array(pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/all_neg2.txt\", header=None , sep=' ')).tolist())\n",
        "  elif itm == 2:\n",
        "    target = np.array(np.array(pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/target_PCA.csv\", header=None)).tolist())\n",
        "    enzyme = np.array(np.array(pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/enzyme_PCA.csv\", header=None)).tolist())\n",
        "    pathway = np.array(np.array(pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/pathway_PCA.csv\", header=None)).tolist())\n",
        "    smile = np.array(np.array(pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/smile_PCA.csv\", header=None)).tolist())\n",
        "  elif itm == 3:\n",
        "    full_pos = np.array(np.vstack((full_pos[:,0],full_pos[:,1],full_pos[:,2],[1]*len(full_pos)))).astype('int32').T\n",
        "    all_cat_pos = []\n",
        "    for i in range(65):\n",
        "      all_cat_pos.append(([np.array(item).tolist() for item in full_pos if item[0]==i]))\n",
        "  elif itm == 4:\n",
        "    l_l = len(all_cat_pos[0])\n",
        "    f_l = 0\n",
        "    all_cat_neg = []\n",
        "    for i in range(65):\n",
        "      all_cat_neg.append(np.vstack(([i]* len(all_cat_pos[i]),all_neg[f_l:l_l,0].tolist(),all_neg[f_l:l_l,1].tolist(),[0]* len(all_cat_pos[i]))).T)\n",
        "      f_l = l_l\n",
        "      if i<64:\n",
        "        l_l += len(all_cat_pos[i+1])\n",
        "  elif itm == 5:\n",
        "    train_cat_pos = []\n",
        "    train_cat_neg = []\n",
        "    valid_cat_pos = []\n",
        "    valid_cat_neg = []\n",
        "    test_cat_pos = []\n",
        "    test_cat_neg = []\n",
        "    for i in range(65):\n",
        "      train_cat_pos.append((all_cat_pos[i][:(len(all_cat_pos[i])*65//100)]))\n",
        "      train_cat_neg.append((all_cat_neg[i][:(len(all_cat_neg[i])*65//100)]))\n",
        "      valid_cat_pos.append((all_cat_pos[i][(len(all_cat_pos[i])*65//100):(len(all_cat_pos[i])*80//100)]))\n",
        "      valid_cat_neg.append((all_cat_neg[i][(len(all_cat_neg[i])*65//100):(len(all_cat_neg[i])*80//100)]))\n",
        "      test_cat_pos.append((all_cat_pos[i][(len(all_cat_pos[i])*80//100):]))\n",
        "      test_cat_neg.append((all_cat_neg[i][(len(all_cat_neg[i])*80//100):]))\n",
        "    train_cat_pos1 = np.array([ii for i in train_cat_pos for ii in i ])\n",
        "    train_cat_neg1 = np.array([ii for i in train_cat_neg for ii in i ])\n",
        "    valid_cat_pos1 = np.array([ii for i in valid_cat_pos for ii in i ])\n",
        "    valid_cat_neg1 = np.array([ii for i in valid_cat_neg for ii in i ])\n",
        "    test_cat_pos1 = np.array([ii for i in test_cat_pos for ii in i ])\n",
        "    test_cat_neg1 = np.array([ii for i in test_cat_neg for ii in i ])\n",
        "  elif itm == 6:\n",
        "    # train_cat = np.array(np.vstack((train_cat_pos1,train_cat_neg1)))\n",
        "    valid_cat = np.array(np.vstack((valid_cat_pos1,valid_cat_neg1)))\n",
        "    test_cat = np.array(np.vstack((test_cat_pos1,test_cat_neg1)))\n",
        "    # train_final = np.array(train_cat[:,:3])\n",
        "    train_final = np.array(train_cat_pos1[:,:3])\n",
        "  elif itm == 7:\n",
        "    m1 = np.array(target).astype(np.float64)\n",
        "    m2 = np.array(enzyme).astype(np.float64)\n",
        "    m3 = np.array(pathway).astype(np.float64)\n",
        "    m4 = np.array(smile).astype(np.float64)\n",
        "    f_all_m1 = np.array(np.column_stack((drugs[:,0],m1)))\n",
        "    f_all_m2 = np.array(np.column_stack((drugs[:,0],m2)))\n",
        "    f_all_m3 = np.array(np.column_stack((drugs[:,0],m3)))\n",
        "    f_all_m4 = np.array(np.column_stack((drugs[:,0],m4)))\n",
        "    print(len(f_all_m1[:]),len(f_all_m1[0]))\n",
        "  else:\n",
        "    print(\"\\n################# DDI copmleted ##################\")\n",
        "    print(\"################# featuers copmleted ##################\")\n",
        "\n",
        "print(test_cat_pos1.shape,valid_cat_pos1.shape,train_cat_pos1.shape,train_cat_pos1[0],valid_cat_pos1[0])\n",
        "tr = []\n",
        "print(\" event >> valid : test >> whate events in valid : whate events in test \")\n",
        "for i in range(572):\n",
        "  s1 = len(train_cat_pos1[np.where(train_cat_pos1[:,1:]==i)])\n",
        "  s2 = len(valid_cat_pos1[np.where(valid_cat_pos1[:,1:]==i)])\n",
        "  s3 = len(test_cat_pos1[np.where(test_cat_pos1[:,1:]==i)])\n",
        "  if s1 == 0:\n",
        "    vid = valid_cat_pos1[np.where(valid_cat_pos1[:,1:]==i)[0],:].tolist()\n",
        "    tst = test_cat_pos1[np.where(test_cat_pos1[:,1:]==i)[0],:].tolist()\n",
        "    print(i,\" >>   \",s2,\" : \",s3 ,\" >> \"\n",
        "    ,np.unique(valid_cat_pos1[np.where(valid_cat_pos1[:,1:]==i)[0],0])\n",
        "    ,\" : \",np.unique(test_cat_pos1[np.where(test_cat_pos1[:,1:]==i)[0],0])\n",
        "    ,\" >>   \",vid,\" : \",tst )\n",
        "    if not np.isnan(vid).all():\n",
        "      for item in vid:\n",
        "        tr.append(item)\n",
        "    if not np.isnan(tst).all():\n",
        "      for item in tst:\n",
        "        tr.append(item)\n",
        "\n",
        "print(\" missing sample in train : \" , tr)\n",
        "\n",
        "print(\" event >> test : valid\")\n",
        "for i in range(572):\n",
        "  s4 = len(test_cat_neg1[np.where(test_cat_neg1[:,1:]==i)])\n",
        "  s5 = len(valid_cat_neg1[np.where(valid_cat_neg1[:,1:]==i)])\n",
        "  if s4 == 0 or s5 == 0:\n",
        "    print(i,\" >>   \",s4,\" : \",s5 )\n",
        "\n",
        "s1, s2, s3, s4, s5 ,s_all ,l_all ,persnt ,sal= [], [], [], [], [], [], [], [], 0\n",
        "events = np.unique(full_pos[:,0])\n",
        "for i in events:\n",
        "  if i < 6:\n",
        "    pp = (len(full_pos[np.where(full_pos[:,0]==i)])/len(full_pos))*100\n",
        "    persnt.append(round(pp,1))\n",
        "    l_all.append(\"\"+str(round(pp,1))+\"%\")\n",
        "    s_all.append(len(full_pos[np.where(full_pos[:,0]==i)]))\n",
        "  else:\n",
        "    sal += len(full_pos[np.where(full_pos[:,0]==i)])\n",
        "  s1.append(len(train_cat_pos1[np.where(train_cat_pos1[:,0]==i)]))\n",
        "  s2.append(len(valid_cat_pos1[np.where(valid_cat_pos1[:,0]==i)]))\n",
        "  s3.append(len(test_cat_pos1[np.where(test_cat_pos1[:,0]==i)]))\n",
        "  s4.append(len(test_cat_neg1[np.where(test_cat_neg1[:,0]==i)]))\n",
        "  s5.append(len(valid_cat_neg1[np.where(valid_cat_neg1[:,0]==i)]))\n",
        "  # print(i,\" >> \",s1,\" : \",s2 ,\" : \",s3 ,\" : \",s4 ,\" : \",s5 )\n",
        "s_all.append(sal)\n",
        "persnt.append(round(sal/len(full_pos)*100,1))\n",
        "l_all.append(\"\"+str(persnt[-1])+\"%\")"
      ],
      "metadata": {
        "id": "oMxCug1tjaG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we can visualiize the number of samples we have for each interaction/event type."
      ],
      "metadata": {
        "id": "tkhAsGrvR8mB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Statistics of datasets\n",
        "plt.title(\"number of samples in the events\")\n",
        "plt.plot(events, s1, label='data train_pos + ')\n",
        "plt.plot(events, s2, label='data valid_pos + ')\n",
        "plt.plot(events, s3, label='data test_pos + ')\n",
        "plt.plot(events, s4, label='data test_neg - ')\n",
        "plt.plot(events, s5, label='data valid_neg - ')\n",
        "plt.xlabel('event')\n",
        "plt.ylabel('number of samples')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pM_aATktkjqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print('\\n',tr,'\\n',vid,'\\n',tst)\n",
        "print(train_final.shape)\n",
        "tr1 = np.array(tr)\n",
        "\n",
        "# plt.title(\"number of samples in the events\")\n",
        "# plt.bar(l_all, s_all)\n",
        "print(s_all,\" | sum : \",sum(s_all),\" all : \",len(full_pos),\"\\n\",persnt,\" | \",sum(persnt))\n",
        "\n",
        "ddd = np.zeros((7)).tolist()\n",
        "ddd[0] = 0.1\n",
        "myexplode = ddd\n",
        "\n",
        "labels = [\"event \"+str(i+1)+\" : \"+str(j) for i,j in enumerate(l_all)]\n",
        "labels[-1] = \"events 7-65 : \"+str(l_all[-1])\n",
        "# title = plt.title(\"number of samples in the events\")\n",
        "# title.set_ha(\"center\")\n",
        "plt.gca().axis(\"equal\")\n",
        "pie = plt.pie(persnt, labels = l_all, explode = myexplode, startangle=90)\n",
        "plt.legend(pie[0],labels, bbox_to_anchor=(0.83,0.5), loc=\"center\", fontsize=10, bbox_transform=plt.gcf().transFigure)\n",
        "plt.subplots_adjust(left=0.0, bottom=0.1, right=0.6)\n",
        "# image_format = 'svg' # e.g .png, .svg, etc.\n",
        "# image_name = 'event_pie.svg'\n",
        "# plt.savefig(image_name, format=image_format, dpi=1200)\n",
        "\n",
        "print(tr1.shape, tr1)\n",
        "train_final1 = np.concatenate((train_final,tr1[:,:3]))\n",
        "print(train_final1.shape,train_final1[-1])\n",
        "train_final = train_final1"
      ],
      "metadata": {
        "id": "WExn5FVHkr05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save all the proprecessed data into .txt files for next step using\n",
        "df = pd.DataFrame(np.array(train_final))\n",
        "df.to_csv('/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5/train.txt', header=None, index=None, sep=' ')\n",
        "df = pd.DataFrame(np.array(valid_cat))\n",
        "df.to_csv('/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5/valid.txt', header=None, index=None, sep=' ')\n",
        "df = pd.DataFrame(np.array(test_cat))\n",
        "df.to_csv('/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5/test.txt', header=None, index=None, sep=' ')\n",
        "\n",
        "def write_f(a_f,path_f):\n",
        "  print(a_f.shape)\n",
        "  a,b = a_f.shape\n",
        "  b = b-1\n",
        "  with open(path_f, \"w\") as txt_file:\n",
        "    csv.writer(txt_file, delimiter=' ').writerow([a,b])\n",
        "    csv.writer(txt_file, delimiter=' ').writerows(a_f)\n",
        "\n",
        "print(len(f_all_m1[:]),len(f_all_m1[0]), f_all_m1.shape)\n",
        "f1 = write_f(f_all_m1,'/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5/featuers_m1.txt')\n",
        "f2 = write_f(f_all_m2,'/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5/featuers_m2.txt')\n",
        "f3 = write_f(f_all_m3,'/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5/featuers_m3.txt')\n",
        "f4 = write_f(f_all_m4,'/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5/featuers_m4.txt')"
      ],
      "metadata": {
        "id": "CKUhCXTTj3SG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With our feature networks and training splits created, we are ready to create and train our GNN."
      ],
      "metadata": {
        "id": "iOnD4sd1O4mj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model\n",
        "\n",
        "In order to make predictions on DDI type, we need to first use a Graph Neural Network to generate a drug embedding vector based on different drug interaction types and drug attributes, and then aggregate the representation vectors. The drug embedding vectors and the aggregation are implemented as follows:"
      ],
      "metadata": {
        "id": "Ox6SUHp_wP7w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Graph Neural Network"
      ],
      "metadata": {
        "id": "LA6EcjPU38QJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "from pandas import DataFrame\n",
        "import pandas as pd\n",
        "import csv\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from numpy import random\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()"
      ],
      "metadata": {
        "id": "RB1pQgiM_C40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function generates batches of data from pairs and their corresponding neighbors, ensuring a specified batch_size. It iterates through the data, constructing batches of x, y, t, and neigh arrays, which are then yielded for use in model training or evaluation."
      ],
      "metadata": {
        "id": "JIqpKMlP_a4z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batches(pairs, neighbors, batch_size):\n",
        "    n_batches = (len(pairs) + (batch_size - 1)) // batch_size\n",
        "\n",
        "    for idx in range(n_batches):\n",
        "        x, y, t, neigh = [], [], [], []\n",
        "        for i in range(batch_size):\n",
        "            index = idx * batch_size + i\n",
        "            if index >= len(pairs):\n",
        "                break\n",
        "            x.append(pairs[index][0])\n",
        "            y.append(pairs[index][1])\n",
        "            t.append(pairs[index][2])\n",
        "            neigh.append(neighbors[pairs[index][0]])\n",
        "        yield (np.array(x).astype(np.int32), np.array(y).reshape(-1, 1).astype(np.int32), np.array(t).astype(np.int32), np.array(neigh).astype(np.int32))"
      ],
      "metadata": {
        "id": "KFg2Ue0D_GIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The train_model function trains a Graph Neural Network (GNN) model using TensorFlow. It constructs the computational graph, and optimizes the model using Adam optimizer. During training, it creates node sequences using Random Walk and learns embeddings through Skip-Gram. Here we iterate through epochs, evaluating and updating the model's performance. Finally, it stores the trained model embeddings and returns evaluation metrics."
      ],
      "metadata": {
        "id": "v5qBZF-m_vfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#GNN training function\n",
        "def train_model(network_data, feature_dic, log_name, f_num, file_name):\n",
        "    args=parse_args()\n",
        "    vocab, index2word, train_pairs = generate(network_data, args.num_walks, args.walk_length, args.schema, file_name, args.window_size, args.num_workers, args.walk_file)\n",
        "\n",
        "    edge_types = list(network_data.keys())\n",
        "\n",
        "    num_nodes = len(index2word)\n",
        "    edge_type_count = len(edge_types)\n",
        "    epochs = args.epoch\n",
        "    batch_size = args.batch_size\n",
        "    embedding_size = args.dimensions # Dimension of the embedding vector.\n",
        "    embedding_u_size = args.edge_dim\n",
        "    u_num = edge_type_count\n",
        "    num_sampled = args.negative_samples # Number of negative examples to sample.\n",
        "    dim_a = args.att_dim\n",
        "    att_head = 1\n",
        "    neighbor_samples = args.neighbor_samples\n",
        "\n",
        "    neighbors = generate_neighbors(network_data, vocab, num_nodes, edge_types, neighbor_samples)\n",
        "\n",
        "    graph = tf.Graph()\n",
        "\n",
        "    if feature_dic is not None:\n",
        "        feature_dim = len(list(feature_dic.values())[0])\n",
        "        print('feature dimension: ' + str(feature_dim))\n",
        "        features = np.zeros((num_nodes, feature_dim), dtype=np.float32)\n",
        "        for key, value in feature_dic.items():\n",
        "            if key in vocab:\n",
        "                features[vocab[key].index, :] = np.array(value)\n",
        "\n",
        "    with graph.as_default():\n",
        "        global_step = tf.Variable(0, name='global_step', trainable=False)\n",
        "\n",
        "        if feature_dic is not None:\n",
        "            node_features = tf.Variable(features, name='node_features', trainable=False)\n",
        "            feature_weights = tf.Variable(tf.truncated_normal([feature_dim, embedding_size], stddev=1.0))\n",
        "\n",
        "            embed_trans = tf.Variable(tf.truncated_normal([feature_dim, embedding_size], stddev=1.0 / math.sqrt(embedding_size)))\n",
        "            u_embed_trans = tf.Variable(tf.truncated_normal([edge_type_count, feature_dim, embedding_u_size], stddev=1.0 / math.sqrt(embedding_size)))\n",
        "        else:\n",
        "            node_embeddings = tf.Variable(tf.random_uniform([num_nodes, embedding_size], -1.0, 1.0))\n",
        "            node_type_embeddings = tf.Variable(tf.random_uniform([num_nodes, u_num, embedding_u_size], -1.0, 1.0))\n",
        "\n",
        "        trans_weights = tf.Variable(tf.truncated_normal([edge_type_count, embedding_u_size, embedding_size // att_head], stddev=1.0 / math.sqrt(embedding_size)))\n",
        "        trans_weights_s1 = tf.Variable(tf.truncated_normal([edge_type_count, embedding_u_size, dim_a], stddev=1.0 / math.sqrt(embedding_size)))\n",
        "        trans_weights_s2 = tf.Variable(tf.truncated_normal([edge_type_count, dim_a, att_head], stddev=1.0 / math.sqrt(embedding_size)))\n",
        "        nce_weights = tf.Variable(tf.truncated_normal([num_nodes, embedding_size], stddev=1.0 / math.sqrt(embedding_size)))\n",
        "        nce_biases = tf.Variable(tf.zeros([num_nodes]))\n",
        "\n",
        "        # Input data\n",
        "        train_inputs = tf.placeholder(tf.int32, shape=[None])\n",
        "        train_labels = tf.placeholder(tf.int32, shape=[None, 1])\n",
        "        train_types = tf.placeholder(tf.int32, shape=[None])\n",
        "        node_neigh = tf.placeholder(tf.int32, shape=[None, edge_type_count, neighbor_samples])\n",
        "\n",
        "        # Look up embeddings for nodes\n",
        "        if feature_dic is not None:\n",
        "            node_embed = tf.nn.embedding_lookup(node_features, train_inputs)\n",
        "            node_embed = tf.matmul(node_embed, embed_trans)\n",
        "        else:\n",
        "            node_embed = tf.nn.embedding_lookup(node_embeddings, train_inputs)\n",
        "\n",
        "        if feature_dic is not None:\n",
        "            node_embed_neighbors = tf.nn.embedding_lookup(node_features, node_neigh)\n",
        "            node_embed_tmp = tf.concat([tf.matmul(tf.reshape(tf.slice(node_embed_neighbors, [0, i, 0, 0], [-1, 1, -1, -1]), [-1, feature_dim]), tf.reshape(tf.slice(u_embed_trans, [i, 0, 0], [1, -1, -1]), [feature_dim, embedding_u_size])) for i in range(edge_type_count)], axis=0)\n",
        "            node_type_embed = tf.transpose(tf.reduce_mean(tf.reshape(node_embed_tmp, [edge_type_count, -1, neighbor_samples, embedding_u_size]), axis=2), perm=[1,0,2])\n",
        "        else:\n",
        "            node_embed_neighbors = tf.nn.embedding_lookup(node_type_embeddings, node_neigh)\n",
        "            node_embed_tmp = tf.concat([tf.reshape(tf.slice(node_embed_neighbors, [0, i, 0, i, 0], [-1, 1, -1, 1, -1]), [1, -1, neighbor_samples, embedding_u_size]) for i in range(edge_type_count)], axis=0)\n",
        "            node_type_embed = tf.transpose(tf.reduce_mean(node_embed_tmp, axis=2), perm=[1,0,2])\n",
        "\n",
        "        trans_w = tf.nn.embedding_lookup(trans_weights, train_types)\n",
        "        trans_w_s1 = tf.nn.embedding_lookup(trans_weights_s1, train_types)\n",
        "        trans_w_s2 = tf.nn.embedding_lookup(trans_weights_s2, train_types)\n",
        "\n",
        "        attention = tf.reshape(tf.nn.softmax(tf.reshape(tf.matmul(tf.tanh(tf.matmul(node_type_embed, trans_w_s1)), trans_w_s2), [-1, u_num])), [-1, att_head, u_num])\n",
        "        node_type_embed = tf.matmul(attention, node_type_embed)\n",
        "        node_embed = node_embed + tf.reshape(tf.matmul(node_type_embed, trans_w), [-1, embedding_size])\n",
        "\n",
        "        if feature_dic is not None:\n",
        "            node_feat = tf.nn.embedding_lookup(node_features, train_inputs)\n",
        "            node_embed = node_embed + tf.matmul(node_feat, feature_weights)\n",
        "\n",
        "        last_node_embed = tf.nn.l2_normalize(node_embed, axis=1)\n",
        "\n",
        "        loss = tf.reduce_mean(\n",
        "            tf.nn.nce_loss(\n",
        "                weights=nce_weights,\n",
        "                biases=nce_biases,\n",
        "                labels=train_labels,\n",
        "                inputs=last_node_embed,\n",
        "                num_sampled=num_sampled,\n",
        "                num_classes=num_nodes))\n",
        "        plot_loss = tf.summary.scalar(\"loss\", loss)\n",
        "\n",
        "        # Optimizer.\n",
        "        optimizer = tf.train.AdamOptimizer().minimize(loss, global_step=global_step)\n",
        "\n",
        "        # Add ops to save and restore all the variables.\n",
        "        # saver = tf.train.Saver(max_to_keep=20)\n",
        "\n",
        "        merged = tf.summary.merge_all(key=tf.GraphKeys.SUMMARIES)\n",
        "\n",
        "        # Initializing the variables\n",
        "        init = tf.global_variables_initializer()\n",
        "\n",
        "    # Launch the graph\n",
        "    print(\"Optimizing\")\n",
        "\n",
        "    with tf.Session(graph=graph) as sess:\n",
        "        writer = tf.summary.FileWriter(\"./runs/\" + log_name, sess.graph) # tensorboard --logdir=./runs\n",
        "        sess.run(init)\n",
        "\n",
        "        print('Training')\n",
        "        g_iter = 0\n",
        "        best_score = 0\n",
        "        test_score = (0.0, 0.0, 0.0)\n",
        "        patience = 0\n",
        "\n",
        "        # Iterate through epochs for training\n",
        "        for epoch in range(epochs):\n",
        "            random.shuffle(train_pairs)\n",
        "            batches = get_batches(train_pairs, neighbors, batch_size)\n",
        "            # Initialize tqdm progress bar for visual feedback during training\n",
        "            data_iter = tqdm(batches,\n",
        "                            desc=\"epoch %d\" % (epoch),\n",
        "                            total=(len(train_pairs) + (batch_size - 1)) // batch_size,\n",
        "                            bar_format=\"{l_bar}{r_bar}\")\n",
        "            avg_loss = 0.0\n",
        "\n",
        "            for i, data in enumerate(data_iter):\n",
        "                feed_dict = {train_inputs: data[0], train_labels: data[1], train_types: data[2], node_neigh: data[3]}\n",
        "                _, loss_value, summary_str = sess.run([optimizer, loss, merged], feed_dict)\n",
        "\n",
        "                writer.add_summary(summary_str, g_iter)\n",
        "\n",
        "                g_iter += 1\n",
        "\n",
        "                avg_loss += loss_value\n",
        "\n",
        "                if i % 5000 == 0:\n",
        "                    post_fix = {\n",
        "                        \"epoch\": epoch,\n",
        "                        \"iter\": i,\n",
        "                        \"avg_loss\": avg_loss / (i + 1),\n",
        "                        \"loss\": loss_value\n",
        "                    }\n",
        "                    data_iter.write(str(post_fix))\n",
        "\n",
        "            # After each epoch, evaluate the model on validation and testing datasets\n",
        "            final_model = dict(zip(edge_types, [dict() for _ in range(edge_type_count)]))\n",
        "            for i in range(edge_type_count):\n",
        "                for j in range(num_nodes):\n",
        "                    final_model[edge_types[i]][index2word[j]] = np.array(sess.run(last_node_embed, {train_inputs: [j], train_types: [i], node_neigh: [neighbors[j]]})[0])\n",
        "\n",
        "            valid_aucs, valid_f1s, valid_prs = [], [], []\n",
        "            test_aucs, test_f1s, test_prs = [], [], []\n",
        "            for i in range(edge_type_count):\n",
        "                if args.eval_type == 'all' or edge_types[i] in args.eval_type.split(','):\n",
        "                    tmp_auc, tmp_f1, tmp_pr = evaluate(final_model[edge_types[i]], valid_true_data_by_edge[edge_types[i]], valid_false_data_by_edge[edge_types[i]])\n",
        "                    valid_aucs.append(tmp_auc)\n",
        "                    valid_f1s.append(tmp_f1)\n",
        "                    valid_prs.append(tmp_pr)\n",
        "\n",
        "                    tmp_auc, tmp_f1, tmp_pr = evaluate(final_model[edge_types[i]], testing_true_data_by_edge[edge_types[i]], testing_false_data_by_edge[edge_types[i]])\n",
        "                    test_aucs.append(tmp_auc)\n",
        "                    test_f1s.append(tmp_f1)\n",
        "                    test_prs.append(tmp_pr)\n",
        "            print('valid auc:', np.mean(valid_aucs))\n",
        "            print('valid pr:', np.mean(valid_prs))\n",
        "            print('valid f1:', np.mean(valid_f1s))\n",
        "\n",
        "            # Calculate average metrics on testing dataset\n",
        "            average_auc = np.mean(test_aucs)\n",
        "            average_f1 = np.mean(test_f1s)\n",
        "            average_pr = np.mean(test_prs)\n",
        "\n",
        "            # Check if current validation score is better than the best score so far\n",
        "            cur_score = np.mean(valid_aucs)\n",
        "            if cur_score > best_score:\n",
        "                best_score = cur_score\n",
        "                test_score = (average_auc, average_f1, average_pr)\n",
        "                patience = 0\n",
        "            else:\n",
        "                patience += 1\n",
        "                if patience > args.patience:\n",
        "                    print('Early Stopping')\n",
        "                    break\n",
        "\n",
        "        # After training, store the final model embeddings in a DataFrame and save to CSV\n",
        "        final_modelss=[]\n",
        "        for i in range(edge_type_count):\n",
        "          for j in range(num_nodes):\n",
        "            final_modelss.append([edge_types[i],index2word[j],final_model[edge_types[i]][index2word[j]]])\n",
        "        df = pd.DataFrame((final_modelss))\n",
        "        df.to_csv(file_name+'/final_modelss'+str(f_num)+'_d_'+str(embedding_size)+'.csv', header=None, index=None) #, sep=' '\n",
        "    return test_score\n",
        "\n"
      ],
      "metadata": {
        "id": "4S8iTVF7246_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "This function loads, trains, and evaluates the model by parsing arguments and loads feature data. After training, it computes metrics like ROC-AUC, PR-AUC, and F1 score, offering insights into model performance."
      ],
      "metadata": {
        "id": "jncRc-aF7VV1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#GNN training and output embeddings for each created feature\n",
        "\n",
        "def generateEmbeddings(input,features,epoch,dimensions):\n",
        "    args = parse_args()\n",
        "    file_name = input\n",
        "    print(args)\n",
        "    if features is not None:\n",
        "        feature_dic = load_feature_data(features)\n",
        "        f_num = str(features[-5])\n",
        "    else:\n",
        "        feature_dic = None\n",
        "        f_num = str(\"dr\")\n",
        "    log_name = file_name.split('/')[-1] + f'_evaltype_{args.eval_type}_b_{args.batch_size}_e_{epoch}'\n",
        "\n",
        "    training_data_by_type = load_training_data(file_name + '/train.txt')\n",
        "    valid_true_data_by_edge, valid_false_data_by_edge = load_testing_data(file_name + '/valid.txt')\n",
        "    testing_true_data_by_edge, testing_false_data_by_edge = load_testing_data(file_name + '/test.txt')\n",
        "\n",
        "    average_auc, average_f1, average_pr = train_model(training_data_by_type, feature_dic, log_name + '_' + time.strftime('%Y-%m-%d %H-%M-%S',time.localtime(time.time())),f_num,file_name)\n",
        "\n",
        "    print('Overall ROC-AUC:', average_auc)\n",
        "    print('Overall PR-AUC', average_pr)\n",
        "    print('Overall F1:', average_f1)\n"
      ],
      "metadata": {
        "id": "Y6OadJfE7JQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below we run our GNN training and output the embedding for each feature. Overall, the process took around 10 hours to complete."
      ],
      "metadata": {
        "id": "PuaujRYqSpgs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Already run\n",
        "#generateEmbeddings(\"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5\",\"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5/featuers_m1.txt\",1,32)"
      ],
      "metadata": {
        "id": "3DFxErhTIeJJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Already Run\n",
        "#generateEmbeddings(\"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5\",\"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5/featuers_m2.txt\",1,32)"
      ],
      "metadata": {
        "id": "49JujiAe24zf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Already Run\n",
        "#generateEmbeddings(\"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5\",\"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5/featuers_m3.txt\",1,32)"
      ],
      "metadata": {
        "id": "8QjeW35424qb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Already Run\n",
        "#generateEmbeddings(\"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5\",\"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5/featuers_m4.txt\",1,32)"
      ],
      "metadata": {
        "id": "iy_3nMsSLSW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "After the completion of the Graph Neural Network (GNN) runs, the output embeddings generated from these runs are concatenated and utilized as features in a Deep Neural Network (DNN) model for Drug-Drug Interaction (DDI) prediction.\n"
      ],
      "metadata": {
        "id": "Oh7g9ZFuLX3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Do not run, final files in Evaluation Section\n",
        "from pandas import DataFrame\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "x1=np.array(np.array(pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5/final_modelss1_d_32.csv\", header=None , sep=',')).tolist())\n",
        "x2=np.array(np.array(pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5/final_modelss2_d_32.csv\", header=None , sep=',')).tolist())\n",
        "x3=np.array(np.array(pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5/final_modelss3_d_32.csv\", header=None , sep=',')).tolist())\n",
        "x4=np.array(np.array(pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5/final_modelss4_d_32.csv\", header=None , sep=',')).tolist())\n",
        "\n",
        "\n",
        "def chang_to_array(s):\n",
        "  # print(s.shape)\n",
        "  final_model = []\n",
        "  # print(final_model.shape)\n",
        "  for i in s:\n",
        "    ev = int(i[0])\n",
        "    dr = int(i[1])\n",
        "    h = i[2].replace('[',\"\")\n",
        "    h = h.replace('...',\"\")\n",
        "    h = h.replace('\\n',\"\")\n",
        "    h = h.replace(']',\"\")\n",
        "    h = h.replace('  ',\" \")\n",
        "    h = h.replace('  ',\" \")\n",
        "    h = h.replace('  ',\" \")\n",
        "    h = h.replace('  ',\" \")\n",
        "    h = h.replace('  ',\" \")\n",
        "    h = h.replace('  ',\" \")\n",
        "    h = h.split(\" \")\n",
        "    # h.pop(0)\n",
        "    for dd,d in enumerate(h):\n",
        "      try:\n",
        "        d = float(d)\n",
        "      except:\n",
        "        h.remove(d)\n",
        "    h = np.array(h, dtype=np.float64)\n",
        "    # print(\"ff : \",final_model[i,j])\n",
        "    con = np.concatenate(([ev,dr],h))\n",
        "    final_model.append(con)\n",
        "  return np.array(final_model)\n",
        "\n",
        "x1 = chang_to_array(x1)\n",
        "print(x1.shape)\n",
        "x2 = chang_to_array(x2)\n",
        "print(x2.shape)\n",
        "x3 = chang_to_array(x3)\n",
        "print(x3.shape)\n",
        "x4 = chang_to_array(x4)\n",
        "print(x4.shape)\n",
        "\n",
        "print(int(x1[0][1]))\n",
        "print(len(x1[0,:]))\n",
        "print(np.array(x1[0,:]))\n",
        "\n",
        "def reduc_shape(m):\n",
        "  r = []\n",
        "  for i in range(572):\n",
        "    try:\n",
        "      s2=np.where(m[:,1]==i)[0]\n",
        "      dd = m[s2[0],2:]\n",
        "      for j in s2[1:]:\n",
        "        # dd = np.mean((dd,m[j,2:]), axis=0)\n",
        "        dd = np.concatenate((dd,m[j,2:]))\n",
        "      r.append([i,dd])\n",
        "    except:\n",
        "      print(\"c\")\n",
        "  return np.array(r)\n",
        "\n",
        "\n",
        "def make_dic(x):\n",
        "  s_dic = dict()\n",
        "  r = []\n",
        "  for i in range(572):\n",
        "    try:\n",
        "      s2=np.where(x[:,1]==i)[0]\n",
        "      dd = x[s2[0],2:]\n",
        "      for j in s2[1:]:\n",
        "        # dd = np.mean((dd,m[j,2:]), axis=0)\n",
        "        dd = np.concatenate((dd,x[j,2:]))\n",
        "      # r.append([i,dd])\n",
        "      s_dic[i] = dd\n",
        "    except:\n",
        "      print(\"c\")\n",
        "  return s_dic\n",
        "\n",
        "xs1 = (make_dic(x1))\n",
        "xs2 = (make_dic(x2))\n",
        "xs3 = (make_dic(x3))\n",
        "xs4 = (make_dic(x4))\n",
        "print(len(xs1[0]))\n",
        "\n",
        "all_fectuer = []\n",
        "all_fectuer.append(xs1)\n",
        "all_fectuer.append(xs2)\n",
        "all_fectuer.append(xs3)\n",
        "all_fectuer.append(xs4)\n",
        "all_fectuer = np.array(all_fectuer)\n",
        "print(all_fectuer.shape)\n",
        "print(len(all_fectuer[0]))\n",
        "print(len(all_fectuer[0][0]))\n",
        "print(all_fectuer[0][0][0])\n",
        "# print(all_fectuer[0][336][0])\n",
        "print(type(all_fectuer[0]))\n",
        "\n",
        "full_pos=np.array(np.array(pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/full_pos2.txt\", header=None , sep=' ')).tolist())\n",
        "print(full_pos.shape)\n",
        "\n",
        "DDI = full_pos[:,1:3]\n",
        "print(DDI.shape)\n",
        "print(65*570)\n",
        "\n",
        "print(all_fectuer.shape)\n",
        "f_i1 = all_fectuer[0]\n",
        "f_i2 = all_fectuer[1]\n",
        "f_i3 = all_fectuer[2]\n",
        "f_i4 = all_fectuer[3]\n",
        "\n",
        "new_feature1 = ( np.array(np.multiply(f_i1[d[0]],f_i1[d[1]])).tolist() for d in (DDI) )\n",
        "new_feature2 = ( np.array(np.multiply(f_i2[d[0]],f_i2[d[1]])).tolist() for d in (DDI) )\n",
        "new_feature3 = ( np.array(np.multiply(f_i3[d[0]],f_i3[d[1]])).tolist() for d in (DDI) )\n",
        "new_feature4 = ( np.array(np.multiply(f_i4[d[0]],f_i4[d[1]])).tolist() for d in (DDI) )\n",
        "\n",
        "df = pd.DataFrame(np.array(list(new_feature1)))\n",
        "df.to_csv('/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5/t_c_m_1_32.txt', header=None, index=None, sep=' ')\n",
        "df = pd.DataFrame(np.array(list(new_feature2)))\n",
        "df.to_csv('/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5/t_c_m_2_32.txt', header=None, index=None, sep=' ')\n",
        "df = pd.DataFrame(np.array(list(new_feature3)))\n",
        "df.to_csv('/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5/t_c_m_3_32.txt', header=None, index=None, sep=' ')\n",
        "df = pd.DataFrame(np.array(list(new_feature4)))\n",
        "df.to_csv('/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5/t_c_m_4_32.txt', header=None, index=None, sep=' ')\n",
        "\n",
        "print(len(all_fectuer[0][0]),len(DDI))\n",
        "full_pos = 0\n",
        "x1 ,x2 ,x3 ,x4 ,xx1 ,xx2 ,xx3 ,xx4,xs1 ,xs2 ,xs3 ,xs4 = 0,0,0,0,0,0,0,0,0,0,0,0\n",
        "full_dataframe,f_dataframe,featuers,drugs,a,x=0,0,0,0,0,0"
      ],
      "metadata": {
        "id": "C0ipIa-TybeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Concat() was run to create /content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5/t_c_m_{1-4}_32.txt\n",
        "#concat()"
      ],
      "metadata": {
        "id": "OS4MWteyQeOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After aggregating the representation vectors, it's time to pass the aggregated vectors into the deep neural network learning model for predictions."
      ],
      "metadata": {
        "id": "94vput-Pwu9b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deep Neural Network\n"
      ],
      "metadata": {
        "id": "VZPrWzO44CO-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this is the deep neural network learning model\n",
        "\n",
        "def DNN():\n",
        "    train_input = Input(shape=(vector_size,), name='Inputlayer')\n",
        "    train_in = Dense(512, activation='relu')(train_input)\n",
        "    train_in = BatchNormalization()(train_in)\n",
        "    train_in = Dropout(droprate)(train_in)\n",
        "    train_in = Dense(256, activation='relu')(train_in)\n",
        "    train_in = BatchNormalization()(train_in)\n",
        "    train_in = Dropout(droprate)(train_in)\n",
        "    train_in = Dense(event_num)(train_in)\n",
        "    out = Activation('softmax')(train_in)\n",
        "    model = Model(inputs=train_input, outputs=out)\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) # binary_crossentropy\n",
        "    return model"
      ],
      "metadata": {
        "id": "dAgMxRTwQAIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training\n",
        "\n",
        "In this paper [1], the DNN model is trained with 5-fold cross validation techique with 4 subsets for training and 1 subset for testing. In the training process, the Adam Optimizer is uesd as optimizer function, and the Categorical cross-entropy loss is used as loss function.\n",
        "\n",
        "- Hyperparameters -\n",
        "    - Optimizer: Adam Optimizer\n",
        "    - Learning Rate: Set to 0.001\n",
        "    - Batch Size: The batch size used for training is 128.\n",
        "    - Hidden Layer Sizes: The neural network architecture consists of two hidden layers with 512 and 256 units respectively.\n",
        "    - Dropout Rate: Dropout rate is set to 0.3 and passed to the DNN() model.\n",
        "    \n",
        "- Computational Requirements -\n",
        "    - Number of Training Epochs: Set to 100.\n",
        "    - Average runtime for each epoch: the runtime for each epoch varies from 8s to 12s, with an averaged 9s.\n",
        "    - Total number of trials: 5 trials in training with cross validation."
      ],
      "metadata": {
        "id": "pdX26jXNRl7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cross_validation(feature_matrix, label_matrix, clf_type, event_num, seed, CV):\n",
        "    all_eval_type = 11\n",
        "    result_all = np.zeros((all_eval_type, 1), dtype=float)\n",
        "    each_eval_type = 6\n",
        "    result_eve = np.zeros((event_num, each_eval_type), dtype=float)\n",
        "    y_true = np.array([])\n",
        "    y_pred = np.array([])\n",
        "    y_score = np.zeros((0, event_num), dtype=float)\n",
        "    index_all_class = get_index(label_matrix, event_num, seed, CV)\n",
        "    matrix = []\n",
        "    if type(feature_matrix) != list:\n",
        "        matrix.append(feature_matrix)\n",
        "        feature_matrix = matrix\n",
        "\n",
        "    for k in range(CV):\n",
        "        print(\"k : \",k)\n",
        "        train_index = np.where(index_all_class != k)\n",
        "        test_index = np.where(index_all_class == k)\n",
        "        pred = np.zeros((len(test_index[0]), event_num), dtype=float)\n",
        "        # dnn=DNN()\n",
        "        for i in range(len(feature_matrix)):\n",
        "            print(\"f : \",i)\n",
        "            xx = bring_f(str(feature_matrix[i]))\n",
        "            xx = np.array(xx)\n",
        "            x_train = xx[train_index]\n",
        "            x_test = xx[test_index]\n",
        "            xx = 0\n",
        "            y_train = label_matrix[train_index]\n",
        "            # one-hot encoding\n",
        "            y_train_one_hot = np.array(y_train)\n",
        "            y_train_one_hot = (np.arange(y_train_one_hot.max() + 1) == y_train[:, None]).astype(dtype='float32')\n",
        "            y_test = label_matrix[test_index]\n",
        "            # one-hot encoding\n",
        "            y_test_one_hot = np.array(y_test)\n",
        "            y_test_one_hot = (np.arange(y_test_one_hot.max() + 1) == y_test[:, None]).astype(dtype='float32')\n",
        "            if clf_type == 'DDIMDL':\n",
        "                dnn = DNN()\n",
        "                early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='auto')\n",
        "                dnn.fit(x_train, y_train_one_hot, batch_size=128, epochs=100,\n",
        "                        validation_data=(x_test, y_test_one_hot),\n",
        "                        callbacks=[early_stopping])\n",
        "                x_train = 0\n",
        "                pred += dnn.predict(x_test)\n",
        "                x_test = 0\n",
        "                continue\n",
        "            elif clf_type == 'RF':\n",
        "                clf = RandomForestClassifier(n_estimators=100)\n",
        "            elif clf_type == 'GBDT':\n",
        "                clf = GradientBoostingClassifier()\n",
        "            elif clf_type == 'SVM':\n",
        "                clf = SVC(probability=True)\n",
        "            elif clf_type == 'FM':\n",
        "                clf = GradientBoostingClassifier()\n",
        "            elif clf_type == 'KNN':\n",
        "                clf = KNeighborsClassifier(n_neighbors=4)\n",
        "            else:\n",
        "                clf = LogisticRegression()\n",
        "            clf.fit(x_train, y_train)\n",
        "            pred += clf.predict_proba(x_test)\n",
        "\n",
        "        dnn = 0\n",
        "        pred_score = pred / len(feature_matrix)\n",
        "        pred_type = np.argmax(pred_score, axis=1)\n",
        "        y_true = np.hstack((y_true, y_test))\n",
        "        y_pred = np.hstack((y_pred, pred_type))\n",
        "        y_score = np.row_stack((y_score, pred_score))\n",
        "    return y_pred, y_score, y_true"
      ],
      "metadata": {
        "id": "oudqsZxs758r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Full Model Run and Evaluation.\n",
        "#Do not run, takes ~3 hrs and we already have the output.\n",
        "from numpy.random import seed\n",
        "seed(1)\n",
        "#from tensorflow import set_random_seed\n",
        "#set_random_seed(2)\n",
        "import csv\n",
        "import sqlite3\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import auc\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Dropout, Input, Activation, BatchNormalization, LSTM, MaxPooling1D, Conv1D\n",
        "from keras.callbacks import EarlyStopping\n",
        "# from tf.keras.utils import plot_model\n",
        "import tensorflow as tf\n",
        "\n",
        "full_pos=np.array(np.array(pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/full_pos2.txt\", header=None , sep=' ')).tolist())\n",
        "new_label = []\n",
        "for i in full_pos:\n",
        "    new_label.append(i[0])\n",
        "print('new_label : ',len(new_label),(new_label[0]))\n",
        "DDI = full_pos[:,1:3]\n",
        "new_label = np.array(new_label)\n",
        "\n",
        "event_num = 65\n",
        "droprate = 0.3\n",
        "vector_size = 2080\n",
        "clf = \"DDIMDL\"\n",
        "CV = 5\n",
        "seed = 0\n",
        "f_matrix = [1,2,3,4]\n",
        "featureName = \"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5/G_allf_32_cm\"\n",
        "\n",
        "def get_index(label_matrix, event_num, seed, CV):\n",
        "    index_all_class = np.zeros(len(label_matrix))\n",
        "    for j in range(event_num):\n",
        "        index = np.where(label_matrix == j)\n",
        "        kf = KFold(n_splits=CV, shuffle=True, random_state=seed)\n",
        "        k_num = 0\n",
        "        for train_index, test_index in kf.split(range(len(index[0]))):\n",
        "            index_all_class[index[0][test_index]] = k_num\n",
        "            k_num += 1\n",
        "\n",
        "    return index_all_class\n",
        "\n",
        "def bring_f(f_item):\n",
        "  full_dataframe = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5/t_c_m_\"+f_item+\"_32.txt\", header=None , sep=' ')\n",
        "  x1=np.array(np.array(full_dataframe).tolist())\n",
        "  full_dataframe = 0\n",
        "  print(x1.shape)\n",
        "  return x1.tolist()"
      ],
      "metadata": {
        "id": "KGkuA8W5yMV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation\n",
        "\n",
        "The metrics used in this project [1] include ACC, AUPR, AUC, F1 Score, Precision, and Recall. Among these metrics, AUPR and AUC use the micro metrics and the rest use macro metrics."
      ],
      "metadata": {
        "id": "c3_m-Fsu6ULl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(pred_type, pred_score, y_test, event_num):\n",
        "    all_eval_type = 11\n",
        "    result_all = np.zeros((all_eval_type, 1), dtype=float)\n",
        "    each_eval_type = 6\n",
        "    result_eve = np.zeros((event_num, each_eval_type), dtype=float)\n",
        "    y_one_hot = label_binarize(y_test, classes=np.arange(event_num))\n",
        "    pred_one_hot = label_binarize(pred_type, classes=np.arange(event_num))\n",
        "\n",
        "    precision, recall, th = multiclass_precision_recall_curve(y_one_hot, pred_score)\n",
        "\n",
        "    result_all[0] = accuracy_score(y_test, pred_type)\n",
        "    result_all[1] = roc_aupr_score(y_one_hot, pred_score, average='micro')\n",
        "    result_all[2] = roc_aupr_score(y_one_hot, pred_score, average='macro')\n",
        "    result_all[3] = roc_auc_score(y_one_hot, pred_score, average='micro')\n",
        "    result_all[4] = roc_auc_score(y_one_hot, pred_score, average='macro')\n",
        "    result_all[5] = f1_score(y_test, pred_type, average='micro')\n",
        "    result_all[6] = f1_score(y_test, pred_type, average='macro')\n",
        "    result_all[7] = precision_score(y_test, pred_type, average='micro')\n",
        "    result_all[8] = precision_score(y_test, pred_type, average='macro')\n",
        "    result_all[9] = recall_score(y_test, pred_type, average='micro')\n",
        "    result_all[10] = recall_score(y_test, pred_type, average='macro')\n",
        "    for i in range(event_num):\n",
        "        result_eve[i, 0] = accuracy_score(y_one_hot.take([i], axis=1).ravel(), pred_one_hot.take([i], axis=1).ravel())\n",
        "        result_eve[i, 1] = roc_aupr_score(y_one_hot.take([i], axis=1).ravel(), pred_one_hot.take([i], axis=1).ravel(),\n",
        "                                          average=None)\n",
        "        result_eve[i, 2] = roc_auc_score(y_one_hot.take([i], axis=1).ravel(), pred_one_hot.take([i], axis=1).ravel(),\n",
        "                                         average=None)\n",
        "        result_eve[i, 3] = f1_score(y_one_hot.take([i], axis=1).ravel(), pred_one_hot.take([i], axis=1).ravel(),\n",
        "                                    average='binary')\n",
        "        result_eve[i, 4] = precision_score(y_one_hot.take([i], axis=1).ravel(), pred_one_hot.take([i], axis=1).ravel(),\n",
        "                                           average='binary')\n",
        "        result_eve[i, 5] = recall_score(y_one_hot.take([i], axis=1).ravel(), pred_one_hot.take([i], axis=1).ravel(),\n",
        "                                        average='binary')\n",
        "    return [result_all, result_eve]"
      ],
      "metadata": {
        "id": "eN7aG7tjQWff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Do Not run - long run time\n",
        "start = time.perf_counter()\n",
        "\n",
        "y_pred, y_score, y_true = cross_validation(f_matrix, new_label, clf, event_num, seed, CV)\n",
        "all_result, each_result = evaluate(y_pred, y_score, y_true, event_num)\n",
        "print(\"all_result, each_result \\n\",all_result,'\\n', each_result)\n",
        "save_result(featureName, 'all', clf, all_result)\n",
        "save_result(featureName, 'each', clf, each_result)\n",
        "\n",
        "print(\"time used:\", time.process_time() - start)\n"
      ],
      "metadata": {
        "id": "OUmtBlouA79v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results - (All code runnable)\n",
        "\n",
        "Table of Results -\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1nScqXKxCSYpIfh_aIRAWu_jz_qsoJ_c9\" width=\"900\" height=\"350\">\n"
      ],
      "metadata": {
        "id": "3ktTIcKN5jYD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Results for tested Hypothesis:**\n",
        "  \n",
        "  **1. Impact of Embedding Dimension Size**\n",
        "  - Original Hypothesis: We hypothesized that the size of the embedding dimension would affect the model's performance, with a dimension size of 32 leading to the best accuracy.\n",
        "  - Reproduction Method: To test this, we trained the model using embedding dimension sizes of 16, 32, and 64, mirroring the original paper's approach.\n",
        "  - Evaluation: After evaluating the model's performance, we found that our results differed with the original hypothesis. They had a dimension size of 32 leading to the highest accuracy, while 16 and 32 were nearly exchangeable for us.\n",
        "    \n",
        "  - In this hypothesis, we generated the embedding vectors with different dimensions, including 16, 32, 64. The running for embedding generation under each dimension took 8 to 10 hours. The embedding vectors were stored in the following files:\n",
        "- `\"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5/final_modelss{1-4}_d_16.csv\"`\n",
        "- `\"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5/final_modelss{1-4}_d_32.csv\"`\n",
        "- `\"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5/final_modelss{1-4}_d_64.csv\"`\n",
        "\n",
        "\n",
        "  - After the embedding generation, the embedding vectors were concatenated, this step took around 1 hour for arregation under each dimension. The resulted aggregated representation vactors were stored in the following files:\n",
        "- `\"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5/t\\_c\\_m\\_{1-4}\\_16.txt\"`\n",
        "- `\"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5/t\\_c\\_m\\_{1-4}\\_32.txt\"`\n",
        "- `\"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5/t\\_c\\_m\\_{1-4}\\_64.txt.csv\"`\n",
        "\n",
        "\n",
        "  The {1-4} in each filename stands for the for feature matrices used in generating the embedding, including \"Target\", \"Enzyme\", \"Pathway\", \"Smile\", respectively.\n",
        "\n",
        "  - The prediction results are stored in the following files:\n",
        "- `\"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5/G\\_allf\\_16\\_cm\\_all\\_DDIMDL.csv\"`\n",
        "- `\"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5/G\\_allf\\_32\\_cm\\_all\\_DDIMDL.csv\"`\n",
        "- `\"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5/G\\_allf\\_64\\_cm\\_all\\_DDIMDL.csv`\n"
      ],
      "metadata": {
        "id": "ge7gojyGzYEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Already run\n",
        "#generateEmbeddings(\"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5\",\"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5/featuers_m1.txt\",1,16)\n",
        "\n",
        "#generateEmbeddings(\"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5\",\"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5/featuers_m2.txt\",1,16)\n",
        "\n",
        "#generateEmbeddings(\"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5\",\"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5/featuers_m3.txt\",1,16)\n",
        "\n",
        "#generateEmbeddings(\"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5\",\"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5/featuers_m3.txt\",1,16)\n",
        "\n",
        "#Concat() was run to create /content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5/t_c_m_{1-4}_16.txt\n",
        "\n",
        "#generateEmbeddings(\"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5\",\"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5/featuers_m1.txt\",1,64)\n",
        "\n",
        "#generateEmbeddings(\"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5\",\"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5/featuers_m2.txt\",1,64)\n",
        "\n",
        "#generateEmbeddings(\"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5\",\"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5/featuers_m3.txt\",1,64)\n",
        "\n",
        "#generateEmbeddings(\"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5\",\"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5/featuers_m3.txt\",1,64)\n",
        "\n",
        "#Concat() was run to create /content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5/t_c_m_{1-4}_64.txt"
      ],
      "metadata": {
        "id": "mCRuA6_szMJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run train_16.py to train the model and make predictions with the embedding size of 16\n",
        "# Resutls stored in \"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5/G_allf_16_cm_all_DDIMDL.csv\"\n",
        "\n",
        "#!python  /content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/train_16.py"
      ],
      "metadata": {
        "id": "vGgdsPij3-qv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run train_64.py to train the model and make predictions with the embedding size of 64, results stored in\n",
        "# Resutls stored in \"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5/G_allf_64_cm_all_DDIMDL.csv\"\n",
        "\n",
        "#!python  /content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/train_64.py"
      ],
      "metadata": {
        "id": "4j1unpY-4EQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the results\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "root_path = \"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5/\"\n",
        "# Load the data from CSV files\n",
        "file_paths = [\n",
        "    \"G_allf_16_cm_all_DDIMDL.csv\",\n",
        "    \"G_allf_32_cm_all_DDIMDL.csv\",\n",
        "    \"G_allf_64_cm_all_DDIMDL.csv\"\n",
        "]\n",
        "\n",
        "dfs = []\n",
        "for file_path in file_paths:\n",
        "    df = pd.read_csv(root_path+file_path, header=None)\n",
        "    dfs.append(df)\n",
        "\n",
        "# Define column names\n",
        "columns = [\"ACC\", \"AUPR Micro\", \"AUPR Macro\", \"AUC Micro\", \"AUC Macro\", \"F1 Micro\", \"F1 Macro\", \"Pre Micro\", \"Pre Macro\", \"Recall Micro\", \"Recall Macro\"]\n",
        "\n",
        "# Reshape the data and combine into a single DataFrame\n",
        "results = pd.DataFrame({f\"Dimension {i}\": df[0].values for i, df in zip([16, 32, 64], dfs)}, index=columns)\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(15, 6))\n",
        "results.plot(kind='bar', rot=45)\n",
        "plt.title(\"Comparison of Metrics for Different Embedding Dimensions\")\n",
        "plt.xlabel(\"Metrics\")\n",
        "plt.ylabel(\"Value\")\n",
        "plt.legend(title=\"Embedding Dimension\", bbox_to_anchor=(1, 1), loc='upper left')  # Move legend outside\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ND0yeMq16gQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above results of the proposed model run on representation vectors with different embedding size, our results differ slightly with the paper's. They claim 32 dimensions were far better than 16, while both are comparable in our results. The dimension of 64 of embedding size did generate the lowest Accuracy which is compatible with the paper."
      ],
      "metadata": {
        "id": "jSpBduO0j6hP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Results for tested Hypothesis:**\n",
        "\n",
        "**2. Impact of Different Drug Feature Matrices**\n",
        "- Original Hypothesis: We posited that utilizing different drug feature matrices would lead to varying performance on the proposed model, with the combined feature matrix showing the best performance.\n",
        "- Reproduction Method: To test this hypothesis, we examined the model's performance using different drug feature matrices, including combined feature matrices.\n",
        "- Evaluation: Our results confirmed the hypothesis, showing that the combined feature matrix indeed contributed to the highest predictive performance compared to other matrices.\n",
        "\n",
        "To test this hypothesis, we tried sigle feature matrices of \"Target\", \"Enzyme\", \"Pathway\", \"Smile\" and combinations of them for test.\n",
        "\n",
        "  - The prediction results with different feature matrix are stored in the following files:\n",
        "- `\"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5/G_allf_32_cm_all_DDIMDL_T.csv\"`\n",
        "- `\"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5/G_allf_32_cm_all_DDIMDL_E.csv\"`\n",
        "- `\"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5/G_allf_32_cm_all_DDIMDL_P.csv`\n",
        "- `\"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5/G_allf_32_cm_all_DDIMDL_S.csv`\n",
        "\n"
      ],
      "metadata": {
        "id": "Unli0rsMEa8c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "root_path = \"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5/\"\n",
        "\n",
        "# Load data\n",
        "files = ['G_allf_32_cm_all_DDIMDL_S.csv', 'G_allf_32_cm_all_DDIMDL_E.csv',\n",
        "         'G_allf_32_cm_all_DDIMDL_T.csv', 'G_allf_32_cm_all_DDIMDL_P.csv',\n",
        "         'G_allf_32_cm_all_DDIMDL_E_P.csv', 'G_allf_32_cm_all_DDIMDL_E_S.csv',\n",
        "         'G_allf_32_cm_all_DDIMDL_E_T.csv', 'G_allf_32_cm_all_DDIMDL_P_S.csv',\n",
        "         'G_allf_32_cm_all_DDIMDL_E_P_S.csv', 'G_allf_32_cm_all_DDIMDL_E_P_T.csv',\n",
        "         'G_allf_32_cm_all_DDIMDL_E_S_T.csv', 'G_allf_32_cm_all_DDIMDL_P_S_T.csv',\n",
        "         'G_allf_32_cm_all_DDIMDL.csv']\n",
        "\n",
        "metrics = ['ACC', 'AUPR Micro', 'F1 Macro']\n",
        "colors = ['black', 'blue', 'red']  # Define colors for each metric\n",
        "\n",
        "# Order of metrics in the files\n",
        "metric_order = ['ACC', 'AUPR Micro', 'AUPR', 'AUC Micro', 'AUC Macro', 'F1 Micro', 'F1 Macro', 'Precision Micro', 'Precision Macro', 'Recall Micro', 'Recall Macro']\n",
        "\n",
        "plt.figure()  # Create a single plot for all metrics\n",
        "plt.title(\"Metrics Comparison\")  # Set title for the plot\n",
        "plt.xlabel('Feature Matrix')  # Set x-axis label\n",
        "plt.ylabel('Value')  # Set y-axis label\n",
        "\n",
        "legend_handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='black', markersize=8),\n",
        "                  plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='blue', markersize=8),\n",
        "                  plt.Line2D([0], [0], marker='o', color='w', markerfacecolor='red', markersize=8)]\n",
        "\n",
        "for metric, color in zip(metrics, colors):\n",
        "    metric_values = []\n",
        "    for file in files:\n",
        "        # Load data from CSV\n",
        "        df = pd.read_csv(root_path + file, header=None)  # Assuming no header in files\n",
        "        metric_index = metric_order.index(metric)  # Find index of the metric in order\n",
        "        metric_values.append(df.iloc[metric_index].values)\n",
        "\n",
        "    # Plot the data for all files for this metric\n",
        "    for file, values in zip(files, metric_values):\n",
        "        plt.plot(['GD+S', 'GD+E', 'GD+T', 'GD+P', 'GD+E+P', 'GD+E+S', 'GD+E+T', 'GD+P+S', 'GD+E+P+S','GD+E+P+T','GD+E+S+T','GD+P+S+T', 'GD+S+E+T+P'], metric_values, label=file, color=color, marker='o')\n",
        "\n",
        "plt.legend(legend_handles, ['ACC', 'AUPR', 'F1'], loc='lower right')\n",
        "plt.ylim(0, 1)\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()  # Show the plot with all lines\n"
      ],
      "metadata": {
        "id": "Okzv3B2LEZef"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above results of the proposed model run on different feature matrix, including individual matrix 'S', 'E', 'T', 'P', and different combination of feature matrices. We can tell as adding more and more feature matrics, the model'e performance is inceased. When four of the feature matrics are applied in the proposed model, the performance is the best. This is compatible with the paper."
      ],
      "metadata": {
        "id": "v8YiqJfnksAZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Results for tested Hypothesis:**\n",
        "\n",
        "**3. Comparison with Existing Models:**\n",
        "- Original Hypothesis: We expected that our proposed method would outperform existing models such as DNN, RF, KNN, and LR in predicting DDIs.\n",
        "- Reproduction Method: To validate this hypothesis, we compared the performance of our proposed method with these baseline models using common performance metrics.\n",
        "- Evaluation: Our comparative analysis demonstrated that the proposed method indeed surpassed existing models in predicting DDIs, consistent with our original hypothesis.\n",
        "\n",
        "In our reproducing, we didn't try models like MDNN, CNN-DDI, DANN_DDI, DDIMDL, DeepDDI, DNN which were introduced in other papers. Instead, we chose RF, KNN, LR which can be directly called from sklearn library for comparison.\n",
        "\n",
        "  - The prediction results with different models are stored in the following files:\n",
        "- `\"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5/G_allf_32_cm_all_DDIMDL.csv\"`\n",
        "- `\"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5/G_allf_32_cm_all_RF.csv\"`\n",
        "- `\"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5/G_allf_32_cm_all_KNN.csv`\n",
        "- `\"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5/G_allf_32_cm_all_LR.csv`"
      ],
      "metadata": {
        "id": "zKhOTiSq-RFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the results\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "root_path = \"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5/\"\n",
        "# Load the data from CSV files\n",
        "file_paths = [\n",
        "    \"G_allf_32_cm_all_DDIMDL.csv\",\n",
        "    \"G_allf_32_cm_all_RF.csv\",\n",
        "    \"G_allf_32_cm_all_KNN.csv\",\n",
        "    \"G_allf_32_cm_all_LR.csv\"\n",
        "]\n",
        "\n",
        "dfs = []\n",
        "for file_path in file_paths:\n",
        "    df = pd.read_csv(root_path+file_path, header=None)\n",
        "    dfs.append(df)\n",
        "\n",
        "# Define column names\n",
        "columns = [\"ACC\", \"AUPR Micro\", \"AUPR Macro\", \"AUC Micro\", \"AUC Macro\", \"F1 Micro\", \"F1 Macro\", \"Pre Micro\", \"Pre Macro\", \"Recall Micro\", \"Recall Macro\"]\n",
        "\n",
        "# Reshape the data and combine into a single DataFrame\n",
        "results = pd.DataFrame({f\"{i}\": df[0].values for i, df in zip(['GNN_DDI', 'RF', 'KNN', 'LR'], dfs)}, index=columns)\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(12, 10))\n",
        "results.plot(kind='bar', rot=45)\n",
        "plt.title(\"Comparison of Metrics for Different Models\")\n",
        "plt.xlabel(\"Metrics\")\n",
        "plt.ylabel(\"Value\")\n",
        "plt.legend(title=\"Model\", bbox_to_anchor=(1, 1), loc='upper left')  # Move legend outside\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EtZV3ZSw-Imw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above results of different models, including the proposed model, running on the embedding vectors with dimension 32, we can tell the proposed model provides the best result, with the highest value on every metric. It's notable that the Logistic Regression has difficulty converging during the training and generated the worst performance."
      ],
      "metadata": {
        "id": "D4okDItfk0x2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discussion\n",
        "\n",
        "Reproducing the paper's results predominantly relied on the GitHub code provided by the authors. Leveraging their codebase significantly expedited the replication process, as it served as a comprehensive blueprint for implementing the proposed methodology.\n",
        "\n",
        "The GitHub repository contained the necessary scripts, configuration files, and data, which greatly facilitated the setup and execution of the experiments. Having access to the original implementation allowed for a more straightforward interpretation of the methodology described in the paper.\n",
        "\n",
        "What made the reproduction process relatively easy was the clarity and organization of the GitHub repository. The code was well-documented, with comments explaining the purpose of each function and module. This made it easier to navigate through the codebase and understand its structure.\n",
        "\n",
        "Additionally, the availability of pre-trained models and example datasets enabled quick validation of the code and ensured consistency with the paper's reported results. This provided a solid foundation for building upon and extending the experiments as needed.\n",
        "\n",
        "However, despite the advantages of using the GitHub code, there were still challenges encountered during the reproduction process. Adapting the code to suit google colab and data required careful consideration and sometimes necessitated modifications to the original implementation. Furthermore, troubleshooting issues related to software dependencies, compatibility with different environments, and hardware constraints posed additional hurdles along the way.\n",
        "\n",
        "In summary, while the GitHub code provided a valuable starting point for reproducing the paper's results, the process was not entirely devoid of challenges. Nevertheless, leveraging the provided codebase significantly expedited the replication process and contributed to the overall reproducibility of the experiments.\n",
        "\n",
        "To improve reproducibility, we would suggest the following to the authors or other reproducers:\n",
        "\n",
        "1. Ensure that all necessary code files and functions are provided with clear documentation on how to use them.\n",
        "2. Provide detailed instructions on how to integrate any additional models or components into the existing codebase. Clear guidance can significantly improve the reproducibility of results.\n"
      ],
      "metadata": {
        "id": "O5pY5Isr5rZ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "1.   Al-Rabeah, M.H., Lakizadeh, A. Prediction of drug-drug interaction events using graph neural networks based feature extraction. Sci Rep 12, 15590 (2022). https://doi.org/10.1038/s41598-022-19999-4"
      ],
      "metadata": {
        "id": "WcOq3f055uUE"
      }
    }
  ]
}