{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "sfk8Zrul_E8V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GitHub Link:\n",
        "https://github.com/genggeng88/dl4h_team61\n",
        "\n",
        "To TA who is grading this notebook. We were originally registered with the paper of GD-RNN. However, we can't find the files for dataset generation in that paper. So, we switch to the paper \"Prediction of drug-drug interaction events using graph neural networks based feature extraction (GNN-DDI)\". (We already notified Sayantani who is the TA for our original paper.)"
      ],
      "metadata": {
        "id": "Nsz-tDLawzKj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "* Background of the problem\n",
        "  * Type of problem\n",
        "    * Nowadays, many difficult diseases are treated using drug mixes (Polyparmacy), which is a good approach that utilizes the synergistic effects of drug interactions. However, unplanned DDIs could risk a patient's life because they may cause side effects or perhaps dangerous toxicity. The detection of DDIs becomes much more necessary, but diagnosing DDI on a large number of drug pairs, both in vitro and in vivo, is costly and time-consuming.\n",
        "  * Importance/meaning of solving the problem\n",
        "    * Detecting possible DDIs decreases the incidence of unexpected drug interactions and reduces drug production costs. It also can optimize the drug creation process.\n",
        "  * Difficulty of the problem\n",
        "    *  DDI network can provide vital information about drugs interactions. Furthermore, using an attributed heterogeneous DDIs network that presents the drug's interaction types along with the drug features can better demonstrate the intrinsic characteristics of a drug. However, it is challenging to integrate various features effectively because the drug features might be correlated and contain redundant information.\n",
        "  * State of the art methods and effectiveness\n",
        "    * There are four popular approaches in the DDI prediction field: Similarity-based methods, Matrix Factorization-based methods, network analysis-based methods and Deep Learning-based methods. Most of current methods are developed to predict whether drugs interact or not, but not to predict the DDI events. Even though few researches created strong efforts in event prediction but there is a space for advancement.\n",
        "*  Paper explanation\n",
        "  * what did the paper propose\n",
        "    * The paper proposed a method for predicting DDI and their type (event) based on attributed heterogeneous graph embedding and a deep learning approach.\n",
        "  * what is the innovations of the method\n",
        "    * This paper first built a heterogeneous drug network by integrating the drug properties in each type of DDI. It then made predictions on what kind of interaction is between drugs.\n",
        "  * how well the proposed method work (in its own metrics)\n",
        "    * Models like DDIMDL, CNN-DDI, DANN-DDI, MDNN, RF, KNN, LR were used for performance comparison with GNN-DDI base on the metrics like ACC, AUPR, AUC, F1 Score, Precision, and Recall. GNN-DDI showed the best performance among different models.\n",
        "  * what is the contribution to the reasearch regime\n",
        "    * This paper proposed a new approach in generating heterogeneous drug network. It also helped make predictions on not only drug interaction, but also drug interaction events which can be beneficial to patients and drug production.\n"
      ],
      "metadata": {
        "id": "MQ0sNuMePBXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code comment is used as inline annotations for your coding"
      ],
      "metadata": {
        "id": "ABD4VhFZbehA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scope of Reproducibility:\n",
        "\n",
        "1.   Hypothesis 1: Embedding dimension size impact the model's performance that dimention size of 32 led to the best accuracy while dimension size of 16 and 64 led to slightly inferior performance.\n",
        "2.   Hypothesis 2: Different integration schemas of drug vectors impact the model's performance. The method concatenates each drug embedding vector in all event types and then multiplies two vectors of drugs pair shows the best performance.\n",
        "3.   Using different drug feature matrix (similarity matrices) displays different performance on the proposed model. The combined feature matrix show the best performance.\n",
        "4.   The proposed method shows the best performance among approaches like MDNN, DDIMDL, CNN-DDI, DANN-DDI, MDNN, DeepDDI, DNN, RF, KNN, and LR. (We may only test with models DNN, RF, KNN, and LR since the rest models are not standard and can't be called from common libraries)"
      ],
      "metadata": {
        "id": "uygL9tTPSVHB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methodology\n",
        "\n",
        "This methodology is the core of your project. It consists of run-able codes with necessary annotations to show the expeiment you executed for testing the hypotheses.\n",
        "\n",
        "The methodology at least contains two subsections **data** and **model** in your experiment."
      ],
      "metadata": {
        "id": "xWAHJ_1CdtaA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import  packages you need\n",
        "import numpy as np\n",
        "from google.colab import drive\n"
      ],
      "metadata": {
        "id": "yu61Jp1xrnKk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Data\n",
        "In this project, we obtained the data from a [sqlite3 database](https://github.com/YifanDengWHU/DDIMDL) which was compiled from [DrugBank](https://go.drugbank.com/) 5.1.3 verision. It has 4 tables:\n",
        "1. <b>drug</b> contains 572 kinds of drugs and their features.\n",
        "2. <b>event</b> contains the 37264 DDIs between the 572 kinds of drugs.\n",
        "3. <b>extraction</b> is the process result of NLPProcess. Each interaction is transformed to a tuple: {mechanism, action, drugA, drugB}\n",
        "4. <b>event_numer</b> lists the kinds of DDI events and their occurence frequency."
      ],
      "metadata": {
        "id": "2NbPHUTMbkD3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload the tables from sqlite3 database called \"Event.db\"\n",
        "\n",
        "import sqlite3\n",
        "\n",
        "#Connection to the DB\n",
        "conn = sqlite3.connect('/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/event.db')\n",
        "\n",
        "cursor = conn. cursor()\n",
        "cursor. execute(\"SELECT name FROM sqlite_master WHERE type='table';\") # [('event_number',), ('event',), ('drug',), ('extraction',)]\n",
        "print(\"Tables : \",cursor. fetchall())\n",
        "cursor. execute(\"SELECT name FROM PRAGMA_TABLE_INFO('drug');\") # [('index',), ('id',), ('target',), ('enzyme',), ('pathway',), ('smile',), ('name',)]\n",
        "print(\"drug : \",cursor. fetchall())\n",
        "cursor. execute(\"SELECT name FROM PRAGMA_TABLE_INFO('event');\") # [('index',), ('id',), ('target',), ('enzyme',), ('pathway',), ('smile',), ('name',)]\n",
        "print(\"event : \",cursor. fetchall())\n",
        "cursor. execute(\"SELECT COUNT(*) FROM event\") # [('index',), ('id',), ('target',), ('enzyme',), ('pathway',), ('smile',), ('name',)]\n",
        "print(\"event row COUNT: \",cursor. fetchall())\n",
        "\n",
        "# close the DB connection\n",
        "conn.close()"
      ],
      "metadata": {
        "id": "mXWdIh5n_7Oo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate similarity matrices\n",
        "import csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "def save_f(name,mat):\n",
        "  print(name)\n",
        "  print(mat.shape)\n",
        "  df = pd.DataFrame(mat)\n",
        "  df.to_csv('/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/'+str(name)+'.csv', header=None, index=False)\n",
        "\n",
        "# construct similarity matrices from adjacency matrices of the properties useing the Jacquard similarity function\n",
        "def Jaccard(matrix):\n",
        "  matrix = np.mat(matrix)\n",
        "  numerator = matrix * matrix.T\n",
        "  denominator = np.ones(np.shape(matrix)) * matrix.T + matrix * np.ones(np.shape(matrix.T)) - matrix * matrix.T\n",
        "  return numerator / denominator\n",
        "\n",
        "\n",
        "# generate the feature vector\n",
        "def feature_vector(df, feature_name):\n",
        "  all_feature = []\n",
        "  drug_list = np.array(df[feature_name]).tolist()\n",
        "  # Features for each drug, for example, when feature_name is target, drug_list=[\"P30556|P05412\",\"P28223|P46098|……\"]\n",
        "  for i in drug_list:\n",
        "    for each_feature in i.split('|'):\n",
        "      if each_feature not in all_feature:\n",
        "        all_feature.append(each_feature)  # obtain all the features\n",
        "  feature_matrix = np.zeros((len(drug_list), len(all_feature)), dtype=float)\n",
        "  df_feature = DataFrame(feature_matrix, columns=all_feature)  # Consrtuct feature matrices with key of dataframe\n",
        "  for i in range(len(drug_list)):\n",
        "    for each_feature in df[feature_name].iloc[i].split('|'):\n",
        "      df_feature[each_feature].iloc[i] = 1\n",
        "\n",
        "  sim_matrix = np.asarray(Jaccard(np.array(df_feature)))\n",
        "  sim_matrix1 = np.array(sim_matrix)\n",
        "  pca = PCA(n_components=len(sim_matrix1))  # PCA dimension\n",
        "  pca.fit(sim_matrix)\n",
        "  sim_matrix = pca.transform(sim_matrix)\n",
        "\n",
        "  return sim_matrix\n",
        "\n",
        "conn = sqlite3.connect('/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/event.db')\n",
        "df_drug = pd.read_sql('select * from drug;', conn)\n",
        "\n",
        "feature_list = ['target', 'enzyme', 'pathway', 'smile']\n",
        "\n",
        "print(df_drug[feature_list[0]][:2])\n",
        "print(df_drug[:][:2])\n",
        "\n",
        "drugs = df_drug[:]\n",
        "drugs = np.array(np.vstack((df_drug['index'],df_drug['name'])))\n",
        "drugs = drugs.T\n",
        "\n",
        "# save the four individual feature matrix\n",
        "for feature in feature_list:\n",
        "  mat = feature_vector(df_drug, feature)\n",
        "  save_f(feature+\"_PCA\", mat)\n",
        "\n",
        "conn.close()"
      ],
      "metadata": {
        "id": "zk7iyktLh32z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# retrieve records from the extraction table in the database\n",
        "# and onstruct the DDI Matrix\n",
        "\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import itertools\n",
        "\n",
        "conn = sqlite3.connect('/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/event.db')\n",
        "extraction = pd.read_sql('select * from extraction;', conn)\n",
        "mechanism = extraction['mechanism']\n",
        "action = extraction['action']\n",
        "drugA = extraction['drugA']\n",
        "drugB = extraction['drugB']\n",
        "d_label = {}\n",
        "d_event=[]\n",
        "for i in range(len(mechanism)):\n",
        "  d_event.append(mechanism[i]+\" \"+action[i])\n",
        "count={}\n",
        "for i in d_event:\n",
        "  if i in count:\n",
        "    count[i]+=1\n",
        "  else:\n",
        "    count[i]=1\n",
        "list1 = sorted(count.items(), key=lambda x: x[1],reverse=True)\n",
        "for i in range(len(list1)):\n",
        "  d_label[list1[i][0]]=i\n",
        "\n",
        "DDI=[]\n",
        "for i in range(len(d_event)):\n",
        "  DDI.append(np.hstack((d_label[d_event[i]],drugA[i], drugB[i])))\n",
        "\n",
        "mat_DDI = np.array(DDI)\n",
        "key = drugs[:,1]\n",
        "val = drugs[:,0]\n",
        "dic = dict(zip(key,val))\n",
        "postive1 = [dic[item] for item in mat_DDI[:,1]]\n",
        "postive2 = [dic[item] for item in mat_DDI[:,2]]\n",
        "full_pos = np.array(np.vstack((mat_DDI[:,0],postive1,postive2))).astype('int32')\n",
        "full_pos = full_pos.T\n",
        "\n",
        "df = pd.DataFrame(np.array(full_pos).tolist())\n",
        "df.to_csv('/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/full_pos2.txt', header=None, index=None, sep=' ')\n",
        "\n",
        "conn.close()"
      ],
      "metadata": {
        "id": "sUx4yBW6i8et"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate negative pairs (non-interacting drug pairs) from a given matrix of positive pairs (interacting drug pairs).\n",
        "\n",
        "def make_neg_pairs(matrix):\n",
        "  all_pos = np.array(matrix)\n",
        "  s1=np.unique(all_pos[:,0])\n",
        "  s2=np.unique(all_pos[:,1])\n",
        "  s3=set(s1).union(s2)\n",
        "  conncted_drug = sorted(s3)\n",
        "  print(\"there are \", len(s3), \" drugs have connaction out of 572\")\n",
        "  ss = [ii for ii in range(572) if not ii in s3]\n",
        "  print(\"there are \", len(ss), \" drugs without connaction out of 572\")\n",
        "  pairs_false = list()\n",
        "  pairs = list()\n",
        "  comparing = all_pos\n",
        "  print(\"start callcolate combinations ... \")\n",
        "  for dr1,dr2 in itertools.combinations(conncted_drug,2):\n",
        "    d1=np.array([dr1,dr2])\n",
        "    d2=np.array([dr1,dr2])\n",
        "    if dr1 == dr2: continue\n",
        "    else: pairs.append((dr1,dr2))\n",
        "  print(\"all pairs : \",len(pairs))\n",
        "  for dr in tqdm(pairs, desc=\"pairs_false generating : \"):\n",
        "    d1=np.array([dr[0],dr[1]])\n",
        "    d2=np.array([dr[1],dr[0]])\n",
        "    if not (dr[0]==dr[1]):\n",
        "      if not ((d2 == comparing).all(axis=1).any() or (d1 == comparing).all(axis=1).any()):\n",
        "        pairs_false.append([dr[0],dr[1]])\n",
        "  base=[]\n",
        "  base2=[]\n",
        "  for o in tqdm(pairs_false, \"all_neg generating : \"):\n",
        "    if (not any(o[0] in h for h in base)) or (not any(o[1] in h for h in base)):\n",
        "      base.append(o)\n",
        "    else:\n",
        "      base2.append(o)\n",
        "  if len(base) > len(conncted_drug) :\n",
        "    print(\"less base .... !\")\n",
        "  pairs_f1 = np.array(base2)\n",
        "  np.random.shuffle(pairs_f1)\n",
        "  all_neg = np.concatenate((base,pairs_f1[:len(all_pos)-len(base)]),axis=0)\n",
        "  np.random.shuffle(all_neg)\n",
        "  print(\"all_neg.shape : \", all_neg.shape, \"all_pos.shape : \", all_pos.shape)\n",
        "  df = pd.DataFrame(np.array(all_neg).tolist())\n",
        "  df.to_csv('/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/all_neg2.txt', header=None, index=None, sep=' ')\n",
        "  return all_neg\n",
        "\n",
        "all_neg = make_neg_pairs(full_pos[:,1:])"
      ],
      "metadata": {
        "id": "1_zVgYcajOoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare the train and test subsets\n",
        "\n",
        "for itm in tqdm(range(9)):\n",
        "  if itm == 0:\n",
        "    full_pos = np.array(np.array(pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/full_pos2.txt\", header=None , sep=' ')).tolist())\n",
        "  elif itm == 1:\n",
        "    all_neg = np.array(np.array(pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/all_neg2.txt\", header=None , sep=' ')).tolist())\n",
        "  elif itm == 2:\n",
        "    target = np.array(np.array(pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/target_PCA.csv\", header=None)).tolist())\n",
        "    enzyme = np.array(np.array(pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/enzyme_PCA.csv\", header=None)).tolist())\n",
        "    pathway = np.array(np.array(pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/pathway_PCA.csv\", header=None)).tolist())\n",
        "    smile = np.array(np.array(pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/smile_PCA.csv\", header=None)).tolist())\n",
        "  elif itm == 3:\n",
        "    full_pos = np.array(np.vstack((full_pos[:,0],full_pos[:,1],full_pos[:,2],[1]*len(full_pos)))).astype('int32').T\n",
        "    all_cat_pos = []\n",
        "    for i in range(65):\n",
        "      all_cat_pos.append(([np.array(item).tolist() for item in full_pos if item[0]==i]))\n",
        "  elif itm == 4:\n",
        "    l_l = len(all_cat_pos[0])\n",
        "    f_l = 0\n",
        "    all_cat_neg = []\n",
        "    for i in range(65):\n",
        "      all_cat_neg.append(np.vstack(([i]* len(all_cat_pos[i]),all_neg[f_l:l_l,0].tolist(),all_neg[f_l:l_l,1].tolist(),[0]* len(all_cat_pos[i]))).T)\n",
        "      f_l = l_l\n",
        "      if i<64:\n",
        "        l_l += len(all_cat_pos[i+1])\n",
        "  elif itm == 5:\n",
        "    train_cat_pos = []\n",
        "    train_cat_neg = []\n",
        "    valid_cat_pos = []\n",
        "    valid_cat_neg = []\n",
        "    test_cat_pos = []\n",
        "    test_cat_neg = []\n",
        "    for i in range(65):\n",
        "      train_cat_pos.append((all_cat_pos[i][:(len(all_cat_pos[i])*65//100)]))\n",
        "      train_cat_neg.append((all_cat_neg[i][:(len(all_cat_neg[i])*65//100)]))\n",
        "      valid_cat_pos.append((all_cat_pos[i][(len(all_cat_pos[i])*65//100):(len(all_cat_pos[i])*80//100)]))\n",
        "      valid_cat_neg.append((all_cat_neg[i][(len(all_cat_neg[i])*65//100):(len(all_cat_neg[i])*80//100)]))\n",
        "      test_cat_pos.append((all_cat_pos[i][(len(all_cat_pos[i])*80//100):]))\n",
        "      test_cat_neg.append((all_cat_neg[i][(len(all_cat_neg[i])*80//100):]))\n",
        "    train_cat_pos1 = np.array([ii for i in train_cat_pos for ii in i ])\n",
        "    train_cat_neg1 = np.array([ii for i in train_cat_neg for ii in i ])\n",
        "    valid_cat_pos1 = np.array([ii for i in valid_cat_pos for ii in i ])\n",
        "    valid_cat_neg1 = np.array([ii for i in valid_cat_neg for ii in i ])\n",
        "    test_cat_pos1 = np.array([ii for i in test_cat_pos for ii in i ])\n",
        "    test_cat_neg1 = np.array([ii for i in test_cat_neg for ii in i ])\n",
        "  elif itm == 6:\n",
        "    # train_cat = np.array(np.vstack((train_cat_pos1,train_cat_neg1)))\n",
        "    valid_cat = np.array(np.vstack((valid_cat_pos1,valid_cat_neg1)))\n",
        "    test_cat = np.array(np.vstack((test_cat_pos1,test_cat_neg1)))\n",
        "    # train_final = np.array(train_cat[:,:3])\n",
        "    train_final = np.array(train_cat_pos1[:,:3])\n",
        "  elif itm == 7:\n",
        "    m1 = np.array(target).astype(np.float64)\n",
        "    m2 = np.array(enzyme).astype(np.float64)\n",
        "    m3 = np.array(pathway).astype(np.float64)\n",
        "    m4 = np.array(smile).astype(np.float64)\n",
        "    # print(\"m1 : \",len(m1),\" m2 : \",len(m2),\" m3 : \",len(m3),\" m4 : \",len(m4))\n",
        "    f_all_m1 = np.array(np.column_stack((drugs[:,0],m1)))\n",
        "    f_all_m2 = np.array(np.column_stack((drugs[:,0],m2)))\n",
        "    f_all_m3 = np.array(np.column_stack((drugs[:,0],m3)))\n",
        "    f_all_m4 = np.array(np.column_stack((drugs[:,0],m4)))\n",
        "    print(len(f_all_m1[:]),len(f_all_m1[0]))\n",
        "  else:\n",
        "    print(\"\\n################# DDI copmleted ##################\")\n",
        "    print(\"################# featuers copmleted ##################\")\n",
        "\n",
        "print(test_cat_pos1.shape,valid_cat_pos1.shape,train_cat_pos1.shape,train_cat_pos1[0],valid_cat_pos1[0])\n",
        "tr = []\n",
        "print(\" event >> valid : test >> whate events in valid : whate events in test \")\n",
        "for i in range(572):\n",
        "  s1 = len(train_cat_pos1[np.where(train_cat_pos1[:,1:]==i)])\n",
        "  s2 = len(valid_cat_pos1[np.where(valid_cat_pos1[:,1:]==i)])\n",
        "  s3 = len(test_cat_pos1[np.where(test_cat_pos1[:,1:]==i)])\n",
        "  if s1 == 0:\n",
        "    vid = valid_cat_pos1[np.where(valid_cat_pos1[:,1:]==i)[0],:].tolist()\n",
        "    tst = test_cat_pos1[np.where(test_cat_pos1[:,1:]==i)[0],:].tolist()\n",
        "    print(i,\" >>   \",s2,\" : \",s3 ,\" >> \"\n",
        "    ,np.unique(valid_cat_pos1[np.where(valid_cat_pos1[:,1:]==i)[0],0])\n",
        "    ,\" : \",np.unique(test_cat_pos1[np.where(test_cat_pos1[:,1:]==i)[0],0])\n",
        "    ,\" >>   \",vid,\" : \",tst )\n",
        "    if not np.isnan(vid).all():\n",
        "      for item in vid:\n",
        "        tr.append(item)\n",
        "    if not np.isnan(tst).all():\n",
        "      for item in tst:\n",
        "        tr.append(item)\n",
        "\n",
        "print(\" missing sample in train : \" , tr)\n",
        "\n",
        "print(\" event >> test : valid\")\n",
        "for i in range(572):\n",
        "  s4 = len(test_cat_neg1[np.where(test_cat_neg1[:,1:]==i)])\n",
        "  s5 = len(valid_cat_neg1[np.where(valid_cat_neg1[:,1:]==i)])\n",
        "  if s4 == 0 or s5 == 0:\n",
        "    print(i,\" >>   \",s4,\" : \",s5 )\n",
        "\n",
        "s1, s2, s3, s4, s5 ,s_all ,l_all ,persnt ,sal= [], [], [], [], [], [], [], [], 0\n",
        "events = np.unique(full_pos[:,0])\n",
        "for i in events:\n",
        "  if i < 6:\n",
        "    pp = (len(full_pos[np.where(full_pos[:,0]==i)])/len(full_pos))*100\n",
        "    persnt.append(round(pp,1))\n",
        "    l_all.append(\"\"+str(round(pp,1))+\"%\")\n",
        "    s_all.append(len(full_pos[np.where(full_pos[:,0]==i)]))\n",
        "  else:\n",
        "    sal += len(full_pos[np.where(full_pos[:,0]==i)])\n",
        "  s1.append(len(train_cat_pos1[np.where(train_cat_pos1[:,0]==i)]))\n",
        "  s2.append(len(valid_cat_pos1[np.where(valid_cat_pos1[:,0]==i)]))\n",
        "  s3.append(len(test_cat_pos1[np.where(test_cat_pos1[:,0]==i)]))\n",
        "  s4.append(len(test_cat_neg1[np.where(test_cat_neg1[:,0]==i)]))\n",
        "  s5.append(len(valid_cat_neg1[np.where(valid_cat_neg1[:,0]==i)]))\n",
        "  # print(i,\" >> \",s1,\" : \",s2 ,\" : \",s3 ,\" : \",s4 ,\" : \",s5 )\n",
        "s_all.append(sal)\n",
        "persnt.append(round(sal/len(full_pos)*100,1))\n",
        "l_all.append(\"\"+str(persnt[-1])+\"%\")"
      ],
      "metadata": {
        "id": "oMxCug1tjaG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Statistics of datasets\n",
        "plt.title(\"number of samples in the events\")\n",
        "plt.plot(events, s1, label='data train_pos + ')\n",
        "plt.plot(events, s2, label='data valid_pos + ')\n",
        "plt.plot(events, s3, label='data test_pos + ')\n",
        "plt.plot(events, s4, label='data test_neg - ')\n",
        "plt.plot(events, s5, label='data valid_neg - ')\n",
        "plt.xlabel('event')\n",
        "plt.ylabel('number of samples')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "pM_aATktkjqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print('\\n',tr,'\\n',vid,'\\n',tst)\n",
        "print(train_final.shape)\n",
        "tr1 = np.array(tr)\n",
        "\n",
        "# plt.title(\"number of samples in the events\")\n",
        "# plt.bar(l_all, s_all)\n",
        "print(s_all,\" | sum : \",sum(s_all),\" all : \",len(full_pos),\"\\n\",persnt,\" | \",sum(persnt))\n",
        "\n",
        "ddd = np.zeros((7)).tolist()\n",
        "ddd[0] = 0.1\n",
        "myexplode = ddd\n",
        "\n",
        "labels = [\"event \"+str(i+1)+\" : \"+str(j) for i,j in enumerate(l_all)]\n",
        "labels[-1] = \"events 7-65 : \"+str(l_all[-1])\n",
        "# title = plt.title(\"number of samples in the events\")\n",
        "# title.set_ha(\"center\")\n",
        "plt.gca().axis(\"equal\")\n",
        "pie = plt.pie(persnt, labels = l_all, explode = myexplode, startangle=90)\n",
        "plt.legend(pie[0],labels, bbox_to_anchor=(0.83,0.5), loc=\"center\", fontsize=10, bbox_transform=plt.gcf().transFigure)\n",
        "plt.subplots_adjust(left=0.0, bottom=0.1, right=0.6)\n",
        "# image_format = 'svg' # e.g .png, .svg, etc.\n",
        "# image_name = 'event_pie.svg'\n",
        "# plt.savefig(image_name, format=image_format, dpi=1200)\n",
        "\n",
        "print(tr1.shape, tr1)\n",
        "train_final1 = np.concatenate((train_final,tr1[:,:3]))\n",
        "print(train_final1.shape,train_final1[-1])\n",
        "train_final = train_final1"
      ],
      "metadata": {
        "id": "WExn5FVHkr05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save all the proprecessed data into .txt files for next step using\n",
        "df = pd.DataFrame(np.array(train_final))\n",
        "df.to_csv('/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5/train.txt', header=None, index=None, sep=' ')\n",
        "df = pd.DataFrame(np.array(valid_cat))\n",
        "df.to_csv('/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5/valid.txt', header=None, index=None, sep=' ')\n",
        "df = pd.DataFrame(np.array(test_cat))\n",
        "df.to_csv('/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5/test.txt', header=None, index=None, sep=' ')\n",
        "\n",
        "def write_f(a_f,path_f):\n",
        "  print(a_f.shape)\n",
        "  a,b = a_f.shape\n",
        "  b = b-1\n",
        "  with open(path_f, \"w\") as txt_file:\n",
        "    csv.writer(txt_file, delimiter=' ').writerow([a,b])\n",
        "    csv.writer(txt_file, delimiter=' ').writerows(a_f)\n",
        "\n",
        "print(len(f_all_m1[:]),len(f_all_m1[0]), f_all_m1.shape)\n",
        "f1 = write_f(f_all_m1,'/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5/featuers_m1.txt')\n",
        "f2 = write_f(f_all_m2,'/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5/featuers_m2.txt')\n",
        "f3 = write_f(f_all_m3,'/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5/featuers_m3.txt')\n",
        "f4 = write_f(f_all_m4,'/content/drive/MyDrive/Colab Notebooks/GNN-DDI/GNN_DDI/DDI/data5/featuers_m4.txt')"
      ],
      "metadata": {
        "id": "CKUhCXTTj3SG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##   Model\n",
        "The proposed model contains two stages:\n",
        "\n",
        "1. Collect the drugs information from different sources and then integrate them through the formation of an attributed heterogeneous network and generate a drug embedding vector based on different drug interaction types and drug attributes. The following figure provides an overview on the first stage.<br>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1uda8ODPHVBPSWiFK62fiQc1Qij2XsN-M\" width=\"450\" height=\"300\">\n",
        "\n",
        "2. Aggregate the representation vectors then predictions of the DDIs and their events are performed through a deep multi‐model framework. The following figure provides an overview on the second stage.<br>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1BbsygQJHpvRCFetVN67ebZixQTLMJBRd\" width=\"450\" height=\"300\">\n"
      ],
      "metadata": {
        "id": "3muyDPFPbozY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "class MyModel:\n",
        "    def __init__(self, num_nodes, embedding_size, edge_type_count, feature_dim=None):\n",
        "        self.num_nodes = num_nodes\n",
        "        self.embedding_size = embedding_size\n",
        "        self.edge_type_count = edge_type_count\n",
        "        self.feature_dim = feature_dim\n",
        "\n",
        "        self.build_model()\n",
        "\n",
        "    def build_model(self):\n",
        "        self.node_embeddings = tf.Variable(tf.random_uniform([self.num_nodes, self.embedding_size], -1.0, 1.0))\n",
        "        self.trans_weights = tf.Variable(tf.truncated_normal([self.edge_type_count, self.embedding_size, self.embedding_size], stddev=1.0 / np.sqrt(self.embedding_size)))\n",
        "\n",
        "        if self.feature_dim is not None:\n",
        "            self.feature_weights = tf.Variable(tf.truncated_normal([self.feature_dim, self.embedding_size], stddev=1.0 / np.sqrt(self.embedding_size)))\n",
        "\n",
        "    def DNN(self):\n",
        "        model = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(512, activation='relu'),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.Dropout(0.5),\n",
        "            tf.keras.layers.Dense(256, activation='relu'),\n",
        "            tf.keras.layers.BatchNormalization(),\n",
        "            tf.keras.layers.Dropout(0.5),\n",
        "            tf.keras.layers.Dense(self.num_nodes, activation='softmax')\n",
        "        ])\n",
        "\n",
        "        model.compile(optimizer='adam',\n",
        "                      loss='categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def forward(self, inputs, node_neigh):\n",
        "        node_embed = tf.nn.embedding_lookup(self.node_embeddings, inputs)\n",
        "        # Add additional forward steps depending on the model complexity and operations\n",
        "        return node_embed"
      ],
      "metadata": {
        "id": "tDdW8XuQ536Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training\n",
        "\n",
        "In the training process, the Noise-Contrastive Estimation (NCE) loss is used as loss function, and the Adam Optimizer is uesd as optimizer function.\n"
      ],
      "metadata": {
        "id": "pdX26jXNRl7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel()\n",
        "\n",
        "def loss_func(labels, logits):\n",
        "    nce_weights = tf.Variable(tf.truncated_normal([model.num_nodes, model.embedding_size], stddev=1.0 / tf.sqrt(model.embedding_size)))\n",
        "    nce_biases = tf.Variable(tf.zeros([model.num_nodes]))\n",
        "\n",
        "    loss = tf.reduce_mean(\n",
        "        tf.nn.nce_loss(weights=nce_weights,\n",
        "                       biases=nce_biases,\n",
        "                       labels=labels,\n",
        "                       inputs=logits,\n",
        "                       num_sampled=10,  # Number of negative examples to sample\n",
        "                       num_classes=model.num_nodes))\n",
        "    return loss\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "def train_model_one_iter(model, loss_func, optimizer, data):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model.call(data, training=True)\n",
        "        loss = loss_func(data[1], logits)  # Assuming data[1] is train_labels\n",
        "\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    return loss\n",
        "\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    # data was not processed yet\n",
        "    data = (train_inputs, train_labels, train_types, node_neigh)\n",
        "    loss = train_model_one_iter(model, loss_func, optimizer, data)\n",
        "    print(f\"Epoch {epoch+1}, Loss: {loss.numpy():.2f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oudqsZxs758r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation\n",
        "\n",
        "In this paper, the model is evaluated with 5-fold cross validation techique with this 4 subsets were used for training and 1 subset was used for testing. The metrics used in this project include ACC, AUPR, AUC, F1 Score, Precision, and Recall. Among these metrics, AUPR and AUC use the micro metrics and the rest use macro metrics."
      ],
      "metadata": {
        "id": "c3_m-Fsu6ULl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import auc\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "# CV approach\n",
        "def cross_validation(feature_matrix, label_matrix, clf_type, event_num, seed, CV):\n",
        "    all_eval_type = 11\n",
        "    result_all = np.zeros((all_eval_type, 1), dtype=float)\n",
        "    each_eval_type = 6\n",
        "    result_eve = np.zeros((event_num, each_eval_type), dtype=float)\n",
        "    y_true = np.array([])\n",
        "    y_pred = np.array([])\n",
        "    y_score = np.zeros((0, event_num), dtype=float)\n",
        "    index_all_class = get_index(label_matrix, event_num, seed, CV)\n",
        "    matrix = []\n",
        "    if type(feature_matrix) != list:\n",
        "        matrix.append(feature_matrix)\n",
        "        feature_matrix = matrix\n",
        "\n",
        "    for k in range(CV):\n",
        "        print(\"k : \",k)\n",
        "        train_index = np.where(index_all_class != k)\n",
        "        test_index = np.where(index_all_class == k)\n",
        "        pred = np.zeros((len(test_index[0]), event_num), dtype=float)\n",
        "        # dnn=DNN()\n",
        "        for i in range(len(feature_matrix)):\n",
        "            print(\"f : \",i)\n",
        "            xx = bring_f(str(feature_matrix[i]))\n",
        "            xx = np.array(xx)\n",
        "            x_train = xx[train_index]\n",
        "            x_test = xx[test_index]\n",
        "            xx = 0\n",
        "            y_train = label_matrix[train_index]\n",
        "            # one-hot encoding\n",
        "            y_train_one_hot = np.array(y_train)\n",
        "            y_train_one_hot = (np.arange(y_train_one_hot.max() + 1) == y_train[:, None]).astype(dtype='float32')\n",
        "            y_test = label_matrix[test_index]\n",
        "            # one-hot encoding\n",
        "            y_test_one_hot = np.array(y_test)\n",
        "            y_test_one_hot = (np.arange(y_test_one_hot.max() + 1) == y_test[:, None]).astype(dtype='float32')\n",
        "            if clf_type == 'DDIMDL':\n",
        "                dnn = DNN()\n",
        "                early_stopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='auto')\n",
        "                dnn.fit(x_train, y_train_one_hot, batch_size=128, epochs=100,\n",
        "                        validation_data=(x_test, y_test_one_hot),\n",
        "                        callbacks=[early_stopping])\n",
        "                x_train = 0\n",
        "                pred += dnn.predict(x_test)\n",
        "                x_test = 0\n",
        "                continue\n",
        "            elif clf_type == 'RF':\n",
        "                clf = RandomForestClassifier(n_estimators=100)\n",
        "            elif clf_type == 'GBDT':\n",
        "                clf = GradientBoostingClassifier()\n",
        "            elif clf_type == 'SVM':\n",
        "                clf = SVC(probability=True)\n",
        "            elif clf_type == 'FM':\n",
        "                clf = GradientBoostingClassifier()\n",
        "            elif clf_type == 'KNN':\n",
        "                clf = KNeighborsClassifier(n_neighbors=4)\n",
        "            else:\n",
        "                clf = LogisticRegression()\n",
        "            clf.fit(x_train, y_train)\n",
        "            pred += clf.predict_proba(x_test)\n",
        "\n",
        "        dnn = 0\n",
        "        pred_score = pred / len(feature_matrix)\n",
        "        pred_type = np.argmax(pred_score, axis=1)\n",
        "        y_true = np.hstack((y_true, y_test))\n",
        "        y_pred = np.hstack((y_pred, pred_type))\n",
        "        y_score = np.row_stack((y_score, pred_score))\n",
        "    return y_pred, y_score, y_true\n",
        "\n",
        "\n",
        "# Calculate Different Metrics\n",
        "def calculate_metric_score(real_labels,predict_score):\n",
        "    # Evaluate the prediction performance\n",
        "    precision, recall, pr_thresholds = precision_recall_curve(real_labels, predict_score)\n",
        "    aupr_score = auc(recall, precision)\n",
        "    all_F_measure = np.zeros(len(pr_thresholds))\n",
        "    for k in range(0, len(pr_thresholds)):\n",
        "       if (precision[k] + recall[k]) > 0:\n",
        "           all_F_measure[k] = 2 * precision[k] * recall[k] / (precision[k] + recall[k])\n",
        "       else:\n",
        "           all_F_measure[k] = 0\n",
        "    print(\"all_F_measure: \")\n",
        "    print(all_F_measure)\n",
        "    max_index = all_F_measure.argmax()\n",
        "    threshold = pr_thresholds[max_index]\n",
        "    fpr, tpr, auc_thresholds = roc_curve(real_labels, predict_score)\n",
        "    auc_score = auc(fpr, tpr)\n",
        "\n",
        "    f = f1_score(real_labels, predict_score)\n",
        "    print(\"F_measure:\"+str(all_F_measure[max_index]))\n",
        "    print(\"f-score:\"+str(f))\n",
        "    accuracy = accuracy_score(real_labels, predict_score)\n",
        "    precision = precision_score(real_labels, predict_score)\n",
        "    recall = recall_score(real_labels, predict_score)\n",
        "    print('results for feature:' + 'weighted_scoring')\n",
        "    print(    '************************AUC score:%.3f, AUPR score:%.3f, precision score:%.3f, recall score:%.3f, f score:%.3f,accuracy:%.3f************************' % (\n",
        "        auc_score, aupr_score, precision, recall, f, accuracy))\n",
        "    results = [auc_score, aupr_score, precision, recall,  f, accuracy]\n",
        "\n",
        "    return results"
      ],
      "metadata": {
        "id": "JiAz5oTp6-R8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plan for Completion\n",
        "For now, we already obtained the database and preprocessed it to get the the attributed heterogeneous graph of drugs and feature matrices. We also have part of the model class, same as training and evaluation components. Our next step will be further developing the model class and adding other models for comparison, listed as bellow:\n",
        "1. Focus on the embedding process for each drug in each event type to generate the embedding matrix. In this matrix, each vector represents the embedding of that drug in a particular event type.\n",
        "2. Use the concatenation method to reduce the embedding matrices' dimensions into a one-dimensional feature vector which will be used as an input of  a multifully connected deep learning model to predict the DDI types.\n",
        "3. Replenish the training and evaluation process to make it runnable with all metircs mentioned in the paper.\n",
        "4. Apply other models for comparison on performance."
      ],
      "metadata": {
        "id": "NE7dqD8K27Eq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results - Not Yet Completed!\n",
        "In this section, you should finish training your model training or loading your trained model. That is a great experiment! You should share the results with others with necessary metrics and figures.\n",
        "\n",
        "Please test and report results for all experiments that you run with:\n",
        "\n",
        "*   specific numbers (accuracy, AUC, RMSE, etc)\n",
        "*   figures (loss shrinkage, outputs from GAN, annotation or label of sample pictures, etc)\n"
      ],
      "metadata": {
        "id": "_lixF-0p289e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# metrics to evaluate my model\n",
        "\n",
        "# plot figures to better show the results\n",
        "\n",
        "# it is better to save the numbers and figures for your presentation."
      ],
      "metadata": {
        "id": "O-IRa_cT3AuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model comparison - Not Yet Completed!\n",
        "\n"
      ],
      "metadata": {
        "id": "zYsOAgfJ3Eyi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# compare you model with others\n",
        "# you don't need to re-run all other experiments, instead, you can directly refer the metrics/numbers in the paper"
      ],
      "metadata": {
        "id": "r6Qx0Z1C3JOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discussion\n",
        "\n",
        "Reproducing the paper's results was somewhat challenging. While the provided code was helpful, there were some areas where we faced challenges in reproducing. As we picked up the paper late, we still need to work on it to completely reproduce the results. On the positive side, the core GNN model implementation was relatively straightforward to understand and reproduce. However, integrating the DNN into the model and ensuring compatibility with the provided training and evaluation pipelines proved to be challenging.\n",
        "\n",
        "To improve reproducibility, I would suggest the following to the authors or other reproducers:\n",
        "\n",
        "1. Ensure that all necessary code files and functions are provided with clear documentation on how to use them.\n",
        "2. Provide detailed instructions on how to integrate any additional models or components into the existing codebase. Clear guidance can significantly improve the reproducibility of results.\n",
        "\n",
        "In the next phase, we plan to address the remaining challenges in reproducing the results. This may involve filling in any missing code or documentation, and further debugging to ensure that the entire pipeline runs smoothly. Additionally, we will explore ways to validate the reproduced results and potentially contribute improvements or optimizations to the codebase for better reproducibility in the future.\n"
      ],
      "metadata": {
        "id": "x8rjytVT3LZc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "1.   Al-Rabeah, M.H., Lakizadeh, A. Prediction of drug-drug interaction events using graph neural networks based feature extraction. Sci Rep 12, 15590 (2022). https://doi.org/10.1038/s41598-022-19999-4"
      ],
      "metadata": {
        "id": "VHBZjH1Zz05F"
      }
    }
  ]
}